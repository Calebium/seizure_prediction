{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and functions for this script\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import libDataIO as dio\n",
    "import libModelLSTM as LSTM\n",
    "import libUtils as utils\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "# Set up for Gmail outgoing email server for script notifications\n",
    "strGmailSMTPServer = 'smtp.gmail.com'\n",
    "intGmailSMTPPort = 587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter magic commands that should only be run when the code is running\n",
    "# in Jupyter. Set to blnBatchMode to True when running in batch mode \n",
    "blnBatchMode = utils.fnIsBatchMode()\n",
    "\n",
    "if (blnBatchMode):\n",
    "    print('Running in BATCH mode...')\n",
    "    \n",
    "else:\n",
    "    print('Running in INTERACTIVE mode...')\n",
    "    \n",
    "    # Automatically reload modules before code execution\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    # Set plotting style\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more verbose traceback info when an error occurs\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a timestamp that is unique to this run\n",
    "strTimestamp = str(utils.fnGenTimestamp())\n",
    "print('strTimestamp = {}'.format(strTimestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the script name and arguments for reference if in batch mode\n",
    "if (blnBatchMode): print(' '.join(sys.argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log all output messages to a log file if in batch mode\n",
    "if (blnBatchMode):\n",
    "    \n",
    "    strLogDir = './Logs/'  # TODO: Make this into an argument?\n",
    "\n",
    "    # Create a new directory if it does not exist\n",
    "    utils.fnOSMakeDir(strLogDir)\n",
    "\n",
    "    # Saving the original stdout and stderr\n",
    "    objStdout = sys.stdout\n",
    "    objStderr = sys.stderr\n",
    "\n",
    "    strLogFilename = 'runTrainLSTM_' + strTimestamp + '.log'\n",
    "    print('strLogFilename = {}'.format(strLogFilename))\n",
    "\n",
    "    # Open a new log file\n",
    "    objLogFile = open(strLogDir + strLogFilename, 'w')\n",
    "\n",
    "    # Replace stdout and stderr with log file so all print statements will\n",
    "    # be redirected to the log file from this point on\n",
    "    sys.stdout = objLogFile\n",
    "    sys.stderr = objLogFile\n",
    "\n",
    "    datScriptStart = utils.fnNow()\n",
    "    print('Script started on {}'.format(utils.fnGetDatetime(datScriptStart)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up script and training parameters from command line arguments (batch mode)\n",
    "# or hard-coded values in the script\n",
    "if (blnBatchMode):\n",
    "    \n",
    "    # BATCH MODE ONLY: This cell will execute in batch mode and parse the relevant\n",
    "    #                  command line arguments\n",
    "    \n",
    "    import argparse\n",
    "    import json\n",
    "\n",
    "    # Construct argument parser\n",
    "    objArgParse = argparse.ArgumentParser()\n",
    "\n",
    "    # Add arguments to the parser\n",
    "    objArgParse.add_argument('-csv',  '--csvpath',            required = True,                  help = '')\n",
    "    objArgParse.add_argument('-tcsv', '--testcsvpath',        required = False, default = '',   help = '')\n",
    "    \n",
    "    objArgParse.add_argument('-rf',   '--resamplingfreq',     required = False, default = -1,   help = '')\n",
    "    objArgParse.add_argument('-du',   '--subseqduration',     required = False, default = -1,   help = '')\n",
    "    objArgParse.add_argument('-ss',   '--stepsizetimepts',    required = False, default = -1,   help = '')\n",
    "    objArgParse.add_argument('-sss',  '--stepsizestates', type = json.loads, required = False, default = '{}', help = '')  # Example use: -sss '{\"ictal\": 128}'\n",
    "    objArgParse.add_argument('-sw',   '--subwindowfraction',  required = False, default = -1,   help = '')\n",
    "    \n",
    "    objArgParse.add_argument('-smod', '--scalingmode',        required = False, default = -1,   help = '')\n",
    "    objArgParse.add_argument('-smin', '--scaledmin',          required = False, default = -1,   help = '')\n",
    "    objArgParse.add_argument('-smax', '--scaledmax',          required = False, default = 1,    help = '')\n",
    "\n",
    "    objArgParse.add_argument('-vf',   '--valsetfrac',         required = False, default = 0.2,  help = '')\n",
    "    objArgParse.add_argument('-tf',   '--testsetfrac',        required = False, default = 0.1,  help = '')\n",
    "    objArgParse.add_argument('-si',   '--shuffleindices',     required = False, default = True, help = '')\n",
    "    objArgParse.add_argument('-sd',   '--shuffledata',        required = False, default = True, help = '')\n",
    "    objArgParse.add_argument('-bs',   '--batchsize',          required = True,                  help = '')\n",
    "\n",
    "    objArgParse.add_argument('-gpu',  '--gpudevice',          required = False, default = -1,   help = '')\n",
    "\n",
    "    objArgParse.add_argument('-hd',   '--hiddendim',          required = True,                  help = '')\n",
    "    objArgParse.add_argument('-nl',   '--numlayers',          required = True,                  help = '')\n",
    "    objArgParse.add_argument('-os',   '--outputsize',         required = True,                  help = '')\n",
    "    objArgParse.add_argument('-dr',   '--dropprob',           required = False, default = 0.5,  help = '')\n",
    "\n",
    "    objArgParse.add_argument('-cw',   '--classweights', nargs = '+', required = False, default = [], help = '')  # Example use: -cv 1 1 5\n",
    "    objArgParse.add_argument('-opt',  '--optimizer',          required = False, default = 0,    help = '')\n",
    "    \n",
    "    objArgParse.add_argument('-lr',   '--learningrate',       required = True,                  help = '')\n",
    "    objArgParse.add_argument('-ep',   '--numepochs',          required = True,                  help = '')\n",
    "    objArgParse.add_argument('-ve',   '--valperepoch',        required = False, default = 20,   help = '')\n",
    "    objArgParse.add_argument('-gc',   '--gradclip',           required = False, default = 5,    help = '')\n",
    "\n",
    "    objArgParse.add_argument('-lg',   '--emaillogin',         required = False,                 help = '')\n",
    "    objArgParse.add_argument('-pw',   '--emailpasswd',        required = False,                 help = '')\n",
    "    objArgParse.add_argument('-fr',   '--fromemail',          required = False,                 help = '')\n",
    "    objArgParse.add_argument('-to',   '--toemails',           required = False,                 help = '')\n",
    "\n",
    "    # Extract the arguments from the command line\n",
    "    dctArgs = vars(objArgParse.parse_args())\n",
    "\n",
    "    # Convert parameters extract from arguments to their appropriate date types\n",
    "    argCSVPath           = dctArgs['csvpath']\n",
    "    argTestCSVPath       = dctArgs['testcsvpath']\n",
    "    \n",
    "    argResamplingFreq    = int(dctArgs['resamplingfreq'])\n",
    "    argSubSeqDuration    = int(dctArgs['subseqduration'])\n",
    "    argStepSizeTimePts   = int(dctArgs['stepsizetimepts'])\n",
    "    argStepSizeStates    = dctArgs['stepsizestates']\n",
    "    argSubWindowFraction = float(dctArgs['subwindowfraction'])\n",
    "    \n",
    "    argScalingMode       = int(dctArgs['scalingmode'])\n",
    "    argScaledMin         = int(dctArgs['scaledmin'])\n",
    "    argScaledMax         = int(dctArgs['scaledmax'])\n",
    "    \n",
    "    argScalingParams = () if (argScalingMode == -1) else (argScalingMode, (argScaledMin, argScaledMax))\n",
    "    \n",
    "    argValSetFrac        = float(dctArgs['valsetfrac'])\n",
    "    argTestSetFrac       = float(dctArgs['testsetfrac'])\n",
    "    argShuffleIndices    = dctArgs['shuffleindices']\n",
    "    argShuffleData       = dctArgs['shuffledata']\n",
    "    argBatchSize         = int(dctArgs['batchsize'])\n",
    "\n",
    "    argGPUDevice         = int(dctArgs['gpudevice'])\n",
    "\n",
    "    #intFeaturesDim      = intTrainNumChannels           # TODO: Find a way to specify which channels to use\n",
    "    argHiddenDim         = int(dctArgs['hiddendim'])\n",
    "    argNumLayers         = int(dctArgs['numlayers'])\n",
    "    argOutputSize        = int(dctArgs['outputsize'])    # TODO: Extract this from training data (2 states: interictal and preictal)\n",
    "    argDropProb          = float(dctArgs['dropprob'])\n",
    "\n",
    "    argClassWeights      = np.array(dctArgs['classweights'], dtype = np.float32)\n",
    "    argOptimizer         = float(dctArgs['optimizer'])\n",
    "    \n",
    "    argLearningRate      = float(dctArgs['learningrate'])\n",
    "    argNumEpochs         = int(dctArgs['numepochs'])     # Number of epochs (entire training set) to train the model\n",
    "    argValPerEpoch       = int(dctArgs['valperepoch'])   # Number of validation loops per epoch\n",
    "    argGradClip          = float(dctArgs['gradclip'])    # Value at which gradient is clipped\n",
    "\n",
    "    argEmailLogin        = dctArgs['emaillogin']         # Login for email account used for notification\n",
    "    argEmailPasswd       = dctArgs['emailpasswd']        # Passwd for email account used for notification\n",
    "    argFromEmail         = dctArgs['fromemail']          # Email address for account used for notification\n",
    "    argToEmails          = dctArgs['toemails']           # Where to send the email notifications (comma separated for multiple emails)\n",
    "    \n",
    "    print('Running in BATCH mode. Using arguments from command line:')\n",
    "    \n",
    "else:\n",
    "\n",
    "    # INTERACTIVE MODE ONLY: Run this cell only when the code is being run in\n",
    "    #                        Jupyter. It will detect that no arguments have\n",
    "    #                        been parsed from the command line and will apply\n",
    "    #                        the following values instead\n",
    "    \n",
    "    argCSVPath           = './DataCSVs/CHB-MIT/chb20.csv'\n",
    "    \n",
    "    #argTestCSVPath       = ''\n",
    "    argTestCSVPath       = './DataCSVs/CHB-MIT/chb20_Test.csv'\n",
    "    \n",
    "    #argResamplingFreq   = 500    # Sampling rate in Hz (default = -1, use raw data's sampling rate)\n",
    "    argResamplingFreq    = -1     # Sampling rate in Hz (default = -1, use raw data's sampling rate)\n",
    "    argSubSeqDuration    = 1      # Duration of each subsequence in seconds (default = -1, use raw data's segment length)\n",
    "    argStepSizeTimePts   = -1     # Step size of sliding window in time points (default = -1, no sliding window)\n",
    "    argStepSizeStates    = {}     # Step size of sliding window for specific segment states (default = {}, use -ssv value)\n",
    "    argSubWindowFraction = 0.3    # Fraction of sliding window to use to determine segment type (default = -1, entire window)\n",
    "    \n",
    "    argScalingMode       = 1      # Type of scaling to be applied to the preprocessed data\n",
    "    argScaledMin         = -1     # Minimum value of the scaled data\n",
    "    argScaledMax         = 1      # Maximum value of the scaled data\n",
    "    \n",
    "    argScalingParams = () if (argScalingMode == -1) else (argScalingMode, (argScaledMin, argScaledMax))\n",
    "    \n",
    "    argValSetFrac        = 0.2    # Fraction of training set to reserve for validation\n",
    "    argTestSetFrac       = 0.1    # Fraction of training set to reserve for testing\n",
    "    argShuffleIndices    = True   # Randomly shuffle training set sequence indices prior to training\n",
    "    argShuffleData       = True   # Randomly shuffle data batches prior to training\n",
    "    argBatchSize         = 32     # Number of subsequences in a batch\n",
    "\n",
    "    argGPUDevice         = 0      # Which GPU device to use for training\n",
    "\n",
    "    #intFeaturesDim      = intTrainNumChannels  # TODO: Find a way to specify which channels to use\n",
    "    argHiddenDim         = 256    # Number of dimensions of the LSTM hidden layers\n",
    "    argNumLayers         = 2      # Number of LSTM layers\n",
    "    #argOutputSize       = 2      # TODO: Extract this from training data (2 states: interictal and preictal)\n",
    "    argOutputSize        = 3      # TODO: Extract this from training data (3 states: interictal, preictal, and ictal)\n",
    "    argDropProb          = 0.5    # Dropout rate (for regularization)\n",
    "\n",
    "    argClassWeights      = []     # Give weight to each class in loss function\n",
    "    argOptimizer         = 1      # Which optimizer to use (0 = Adam, 1 = AdamW, 2 = SGD. Default = 0)\n",
    "    \n",
    "    argLearningRate      = 0.001  # Learning rate\n",
    "    argNumEpochs         = 1      # Number of trainging epochs\n",
    "    argValPerEpoch       = 20     # Number of validation loops per epoch\n",
    "    argGradClip          = 5      # Value at which gradient is clipped\n",
    "\n",
    "    argEmailLogin        = 'RodentSys'\n",
    "    argFromEmail         = 'RodentSys@gmail.com'\n",
    "    argToEmails          = '4089300606@txt.att.net'\n",
    "    \n",
    "    print('Running in INTERACTIVE mode. Using hard-coded values from script:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out all specified arguments\n",
    "print('argCSVPath = {}'.format(argCSVPath))\n",
    "print('argTestCSVPath = {}'.format(argTestCSVPath))\n",
    "\n",
    "print('argResamplingFreq = {}'.format(argResamplingFreq))\n",
    "print('argSubSeqDuration = {}'.format(argSubSeqDuration))\n",
    "print('argStepSizeTimePts = {}'.format(argStepSizeTimePts))\n",
    "print('argStepSizeStates = {}'.format(argStepSizeStates))\n",
    "print('argSubWindowFraction = {}'.format(argSubWindowFraction))\n",
    "\n",
    "print('argScalingParams = {}'.format(argScalingParams))\n",
    "\n",
    "print('argValSetFrac = {}'.format(argValSetFrac))\n",
    "print('argTestSetFrac = {}'.format(argTestSetFrac))\n",
    "print('argShuffleIndices = {}'.format(argShuffleIndices))\n",
    "print('argShuffleData = {}'.format(argShuffleData))\n",
    "print('argBatchSize = {}'.format(argBatchSize))\n",
    "\n",
    "print('argGPUDevice = {}'.format(argGPUDevice))\n",
    "\n",
    "#print('argFeaturesDim = {}'.format(argFeaturesDim))\n",
    "print('argHiddenDim = {}'.format(argHiddenDim))\n",
    "print('argNumLayers = {}'.format(argNumLayers))\n",
    "print('argOutputSize = {}'.format(argOutputSize))\n",
    "print('argDropProb = {}'.format(argDropProb))\n",
    "\n",
    "print('argClassWeights = {}'.format(argClassWeights))\n",
    "print('argOptimizer = {}'.format(argOptimizer))\n",
    "\n",
    "print('argLearningRate = {}'.format(argLearningRate))\n",
    "print('argNumEpochs = {}'.format(argNumEpochs))\n",
    "print('argValPerEpoch = {}'.format(argValPerEpoch))\n",
    "print('argGradClip = {}'.format(argGradClip))\n",
    "\n",
    "#print('argLoadModel = {}'.format(argLoadModel))\n",
    "\n",
    "print('argEmailLogin = {}'.format(argEmailLogin))\n",
    "print('argEmailPasswd = {}'.format(argEmailPasswd))\n",
    "print('argFromEmail = {}'.format(argFromEmail))\n",
    "print('argToEmails = {}'.format(argToEmails))\n",
    "\n",
    "print()\n",
    "\n",
    "utils.fnShowMemUsage()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#\n",
    "#       (1) Evaluate the effect of data randomization\n",
    "#             (a) Randomized segment indices vs non-randomized\n",
    "#             (b) Shuffling data in DataLoader vs not shuffing\n",
    "#       (2) Compare training with single patient data vs multipls patients\n",
    "#       (3) Compare original sampling rate vs down-sampled rate (implemented)\n",
    "#       (4) Compare subsequences of various lengths\n",
    "#       (5) Investigate between segments with different file sizes\n",
    "#       (6) Train using subsequences from different segments\n",
    "#       (7) Check that all provided EEG segments are of the same voltage\n",
    "#           scale and are centered identically. Center and normalize all\n",
    "#           channels and segments?\n",
    "#       (8) Find the minimum set of channels that can make good predictions\n",
    "#       (9) Investigate the effect of using a moving window to create\n",
    "#           subsequences\n",
    "\n",
    "# NOTE: There are two points of concern of using RNN/LSTM on the Kaggle\n",
    "#       prediction data set\n",
    "#\n",
    "#       (1) Whether we want to randomize the different training segments\n",
    "#           (which are numbered in chronological order) during training.\n",
    "#           Right now we decided not to do that since the point of RNNs\n",
    "#           is to learn from the history of the signal. However, we don't\n",
    "#           know how long this window should be in order to get good\n",
    "#           prediction. The segment length from the data set is 10 mins,\n",
    "#           so we don't know whether it is OK to break up the segments\n",
    "#           into 1 min lengths, or whether it is better to chain up\n",
    "#           multiple segments into something longer than 10 mins\n",
    "#\n",
    "#       (2) No test labels are provided for the data set, presumably due\n",
    "#           to fairness so that everyone is doing a blind test. However,\n",
    "#           this will be a challange on how to evaluate the prediction\n",
    "#           accuracy of the model. Perhaps we should split the raw\n",
    "#           training data into 3 sets for training, validation, and test\n",
    "#\n",
    "#       (3) The human EEGs are sampled at 5000Hz, may consider down-sampling\n",
    "#           to 256 or 500Hz to reduce the data set size, which will also\n",
    "#           reduce the number of parameters required for the model\n",
    "#\n",
    "#           Update: If we do not downsample and use the raw data at 5000Hz,\n",
    "#                   we will run out of GPU memory if we train more than 2\n",
    "#                   segments at the same time (or increase batch size to\n",
    "#                   more than 3, or set the number of hidden dim > 256).\n",
    "#                   Therefore, it makes sense to be able to downsample to\n",
    "#                   make room for training in larger batches or wider\n",
    "#                   layers\n",
    "#\n",
    "#       (4) Each segment is provided as 10 mins long. May want to consider\n",
    "#           breaking each segment up into subsegments (e.g. 1 min segments)\n",
    "#           depending on whether a shorter sequence length is still effective\n",
    "#           for training the model\n",
    "#\n",
    "#       (5) Segments are related and ordered by their sequence numbers. For\n",
    "#           example: interictal segments 0001 - 0006 are 10-min segments from\n",
    "#           the same hour arranged in chronological order. As are segments\n",
    "#           0007 - 0012, 0013 - 0018, 0019 - 0024, 0025 - 0030, 0031 - 0036,\n",
    "#           0037 - 0042, and 0043 - 0048. Segments 0049 - 0050 are 10-min\n",
    "#           segments are adjacent in time. The same applies for the preictal\n",
    "#           segments\n",
    "#\n",
    "#           However, it is not obvious what the time relationship is between\n",
    "#           the interictal and preictal segments\n",
    "#\n",
    "#       (6) Looks like if we train with segments within the samne one-hour\n",
    "#           period, prediction using other segments within the same period is\n",
    "#           pretty good in general if the training went well (no over-fitting\n",
    "#           or under-fitting). However, predictions from other time periods\n",
    "#           are less stellar (around 50%)\n",
    "#\n",
    "#           NOTE: Using the same hyperparameters may train some segments better\n",
    "#                 than others. Do we need to automate the exploration of\n",
    "#                 hyperparameter space for optimized training? Can this be an\n",
    "#                 innovation?\n",
    "#\n",
    "#       (7) If we train multiple segments from different time periods in the\n",
    "#           same training (e.g. train using segments 0001, 0002, 0007, and 0008)\n",
    "#           the prediction result is lower than if we train with segments from\n",
    "#           the same time period\n",
    "\n",
    "# Read CHB-MIT training data from files using sliding window\n",
    "lstTrainingFilenames, lstTrainingSegLabels, lstTrainingSegTypes, arrTrainingDataRaw, lstTrainingSegDurations, lstTrainingSamplingFreqs, lstTrainingChannels, lstTrainingSequences, lstTrainingSubSequences, lstTrainingSeizureDurations, arrTrainingStartEndTimesSec, tupScalingInfo = dio.fnReadCHBMITEDFFiles_SlidingWindow(\n",
    "    argCSVPath = argCSVPath, argTestCSVPath = argTestCSVPath, argResamplingFreq = argResamplingFreq, argSubSeqDuration = argSubSeqDuration, argScalingParams = argScalingParams, argStepSizeTimePts = argStepSizeTimePts, argStepSizeStates = argStepSizeStates, argSubWindowFraction = argSubWindowFraction, argAnnoSuffix = 'annotation.txt', argDebug = True, argTestMode = False)\n",
    "\n",
    "print()\n",
    "\n",
    "utils.fnShowMemUsage()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Take a quick snapshot of the data set\n",
    "intStartIdx = 0\n",
    "intEndIdx = 10\n",
    "#intEndIdx = len(lstTrainingSegLabels)\n",
    "\n",
    "print('Displaying a snapshot of the data set (from subsequence {} to {}):\\n'.format(intStartIdx, intEndIdx))\n",
    "\n",
    "for tupZip in zip(list(range(len(lstTrainingSegLabels[intStartIdx:intEndIdx]))),\n",
    "                  lstTrainingSegLabels[intStartIdx:intEndIdx],\n",
    "                  lstTrainingSegTypes[intStartIdx:intEndIdx],\n",
    "                  lstTrainingSegDurations[intStartIdx:intEndIdx],\n",
    "                  lstTrainingSamplingFreqs[intStartIdx:intEndIdx],\n",
    "                  lstTrainingSequences[intStartIdx:intEndIdx],\n",
    "                  lstTrainingSubSequences[intStartIdx:intEndIdx]):\n",
    "    print(*tupZip, sep = '\\t')\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform non-random oversampling of the ictal data due to imbalanced\n",
    "# classification between the amount of interictal data versus ictal data\n",
    "\n",
    "blnDebug = False\n",
    "\n",
    "# Get the subsequences that are labeled as ictal state\n",
    "lstSeizureSeqIdx = [intSeqIdx for intSeqIdx, intSegType in enumerate(lstTrainingSegTypes) if intSegType == 2]\n",
    "\n",
    "# Collect all related data for these ictal subsequences\n",
    "lstSeizureFilenames     = list(itemgetter(*lstSeizureSeqIdx)(lstTrainingFilenames))\n",
    "lstSeizureSeqLabels     = list(itemgetter(*lstSeizureSeqIdx)(lstTrainingSegLabels))\n",
    "lstSeizureSegTypes      = list(itemgetter(*lstSeizureSeqIdx)(lstTrainingSegTypes))\n",
    "arrSeizureDataRaw       = arrTrainingDataRaw[:, :, lstSeizureSeqIdx]\n",
    "lstSeizureSegDurations  = list(itemgetter(*lstSeizureSeqIdx)(lstTrainingSegDurations))\n",
    "lstSeizureSamplingFreqs = list(itemgetter(*lstSeizureSeqIdx)(lstTrainingSamplingFreqs))\n",
    "lstSeizureChannels      = list(itemgetter(*lstSeizureSeqIdx)(lstTrainingChannels))\n",
    "lstSeizureSequences     = list(itemgetter(*lstSeizureSeqIdx)(lstTrainingSequences))\n",
    "lstSeizureSubSequences  = list(itemgetter(*lstSeizureSeqIdx)(lstTrainingSubSequences))\n",
    "\n",
    "print('arrSeizureDataRaw.shape = {}'.format(arrSeizureDataRaw.shape))\n",
    "print()\n",
    "\n",
    "if (blnDebug):\n",
    "    # Print info on the collected ictal subsequences\n",
    "    for tupSubSeq in zip(lstSeizureFilenames, lstSeizureSeqLabels, lstSeizureSegTypes, lstSeizureSegDurations, lstSeizureSamplingFreqs, lstSeizureSequences, lstSeizureSubSequences):\n",
    "        print('{}, {}, {}, {}, {}, {}, {}'.format(*tupSubSeq))\n",
    "\n",
    "    print()\n",
    "\n",
    "# Calculate the ratio between the number of ictal and non-ictal subsequences,\n",
    "# and multiply the number of ictal subsequences with this imbalance factor to\n",
    "# increase the presence of ictal subsequences in the data set\n",
    "intTotalSubSeqs = arrTrainingDataRaw.shape[2]\n",
    "intNumSeizureSubSeqs = arrSeizureDataRaw.shape[2]\n",
    "\n",
    "intImbalFactor = int(round((intTotalSubSeqs - intNumSeizureSubSeqs) / intNumSeizureSubSeqs))\n",
    "print('intTotalSubSeqs = {}, intNumSeizureSubSeqs = {}: intImbalFactor = {}'.format(intTotalSubSeqs, intNumSeizureSubSeqs, intImbalFactor))\n",
    "print()\n",
    "\n",
    "# Perform oversampling of ictal data only if intImbalFactor > 0\n",
    "if (intImbalFactor > 0):\n",
    "    print('Oversampling of ictal data performed using intImbalFactor = {}'.format(intImbalFactor))\n",
    "    print()\n",
    "    \n",
    "    # Multiply the number of ictal subsequences by intImbalFactor\n",
    "    lstSeizureFilenamesRep     = lstSeizureFilenames * intImbalFactor\n",
    "    lstSeizureSeqLabelsRep     = lstSeizureSeqLabels * intImbalFactor\n",
    "    lstSeizureSegTypesRep      = lstSeizureSegTypes * intImbalFactor\n",
    "    arrSeizureDataRawRep       = np.tile(arrSeizureDataRaw, (1, 1, intImbalFactor))\n",
    "    lstSeizureSegDurationsRep  = lstSeizureSegDurations * intImbalFactor\n",
    "    lstSeizureSamplingFreqsRep = lstSeizureSamplingFreqs * intImbalFactor\n",
    "    lstSeizureChannelsRep      = lstSeizureChannels * intImbalFactor\n",
    "    lstSeizureSequencesRep     = lstSeizureSequences * intImbalFactor\n",
    "    lstSeizureSubSequencesRep  = lstSeizureSubSequences * intImbalFactor\n",
    "\n",
    "    print('arrSeizureDataRawRep.shape = {}'.format(arrSeizureDataRawRep.shape))\n",
    "\n",
    "    #for tupSubSeqRep in zip(lstSeizureFilenamesRep, lstSeizureSeqLabelsRep, lstSeizureSegTypesRep, lstSeizureSegDurationsRep, lstSeizureSamplingFreqsRep, lstSeizureSequencesRep, lstSeizureSubSequencesRep):\n",
    "    #    print('{}, {}, {}, {}, {}, {}, {}'.format(*tupSubSeqRep))\n",
    "\n",
    "    #print()\n",
    "\n",
    "    # Append the replicated ictal data to the end of the data set\n",
    "    lstTrainingFilenamesAug     = lstTrainingFilenames + lstSeizureFilenamesRep\n",
    "    lstTrainingSegLabelsAug     = lstTrainingSegLabels + lstSeizureSeqLabelsRep\n",
    "    lstTrainingSegTypesAug      = lstTrainingSegTypes + lstSeizureSegTypesRep\n",
    "    arrTrainingDataRawAug       = np.concatenate((arrTrainingDataRaw, arrSeizureDataRawRep), axis = 2)\n",
    "    lstTrainingSegDurationsAug  = lstTrainingSegDurations + lstSeizureSegDurationsRep\n",
    "    lstTrainingSamplingFreqsAug = lstTrainingSamplingFreqs + lstSeizureSamplingFreqsRep\n",
    "    lstTrainingChannelsAug      = lstTrainingChannels + lstSeizureChannelsRep\n",
    "    lstTrainingSequencesAug     = lstTrainingSequences + lstSeizureSequencesRep\n",
    "    lstTrainingSubSequencesAug  = lstTrainingSubSequences + lstSeizureSubSequencesRep\n",
    "\n",
    "    print('arrTrainingDataRawAug.shape = {}'.format(arrTrainingDataRawAug.shape))\n",
    "\n",
    "    # Append the replicated ictal data to the end of the data set\n",
    "    lstTrainingFilenames     = lstTrainingFilenamesAug\n",
    "    lstTrainingSegLabels     = lstTrainingSegLabelsAug\n",
    "    lstTrainingSegTypes      = lstTrainingSegTypesAug\n",
    "    arrTrainingDataRaw       = arrTrainingDataRawAug\n",
    "    lstTrainingSegDurations  = lstTrainingSegDurationsAug\n",
    "    lstTrainingSamplingFreqs = lstTrainingSamplingFreqsAug\n",
    "    lstTrainingChannels      = lstTrainingChannelsAug\n",
    "    lstTrainingSequences     = lstTrainingSequencesAug\n",
    "    lstTrainingSubSequences  = lstTrainingSubSequencesAug\n",
    "\n",
    "    print()\n",
    "\n",
    "    print('Size of arrSeizureDataRawRep = {:.2f}Gb'.format(utils.fnByte2GB(arrSeizureDataRawRep.nbytes)))\n",
    "    print('Size of arrTrainingDataRaw   = {:.2f}Gb'.format(utils.fnByte2GB(arrTrainingDataRaw.nbytes)))\n",
    "    print()\n",
    "    \n",
    "else:\n",
    "    print('Oversampling of ictal data not performed since intImbalFactor = {}'.format(intImbalFactor))\n",
    "    print()\n",
    "    \n",
    "    print('Size of arrSeizureDataRaw = {:.2f}Gb'.format(utils.fnByte2GB(arrSeizureDataRaw.nbytes)))\n",
    "    print('Size of arrTrainingDataRaw   = {:.2f}Gb'.format(utils.fnByte2GB(arrTrainingDataRaw.nbytes)))\n",
    "    print()\n",
    "    \n",
    "utils.fnShowMemUsage()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total available CPU memory = 32GB\n",
    "# Total available GPU memory = 11GB\n",
    "\n",
    "# (1) Use stepsize = 100 and train\n",
    "# (2) Try larger window sizes and compare memory usage\n",
    "# (3) Debug by inspecting whether window is slicing correctly\n",
    "# (4) Use false positives and false negatives to determine\n",
    "#     which raw data file to inspect (if prediction accuracy\n",
    "#     is high, even if labeling is incorrect, doesn't matter)\n",
    "\n",
    "# Looks like we're CPU memory-limited instead of GPU resource limited\n",
    "# stepsize = 100, 4.81GB -> 9.66GB (OK) -> theoretical = 18GB, actual = 14GB used (GPU usage = 844MB, 41%)\n",
    "# stepsize = 80, 6.00GB -> 12.06GB (OK) -> theoretical = 24GB, actual = 18GB used (GPU usage = 970MB, 48%)\n",
    "# stepsize = 60, 8.05GB -> 16.14GB (OK) -> theoretical = 32GB, actual = 24GB used (out of memory in training/val set cell)\n",
    "# stepsize = 55, 8.78GB -> 17.60GB (OK) -> theoretical = 34GB, actual = 26GB used (out of memory in training/val set cell)\n",
    "# stepsize = 50, 9.65GB -> ~20GB (not OK) -> theoretical = 40GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input width = 15 (number of channels/features)\n",
    "# Sequence length = 600 * 5000 = 3000000 (number of time points)\n",
    "# Batch size = 1 (for each segment)\n",
    "\n",
    "# Reshape arrTrainingDataRaw[] from [feature/channel size x segment length x batch/segment size]\n",
    "# to batch_first [batch/segment size x segment length x feature/channel size]\n",
    "intTrainNumChannels, intTrainSeqLen, intTrainNumSegments = arrTrainingDataRaw.shape\n",
    "print('intTrainNumChannels, intTrainSeqLen, intTrainNumSegments = ({}, {}, {})'.format(intTrainNumChannels, intTrainSeqLen, intTrainNumSegments))\n",
    "arrTrainingDataBatchFirst = arrTrainingDataRaw.T.reshape(intTrainNumSegments, intTrainSeqLen, intTrainNumChannels)\n",
    "print('arrTrainingDataBatchFirst.shape = {}'.format(arrTrainingDataBatchFirst.shape))\n",
    "\n",
    "# Convert the segment types into an np.array\n",
    "arrTrainingSegTypes = np.array(lstTrainingSegTypes, dtype = int)\n",
    "print('arrTrainingSegTypes = {}'.format(arrTrainingSegTypes))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the original training set into training, validation, and test sets\n",
    "fltValSetFrac = argValSetFrac    # Fraction of segments reserved for validation\n",
    "fltTestSetFrac = argTestSetFrac  # Fraction of segments reserved for testing (useful for Kaggle prediction data set)\n",
    "fltTrainSetFrac = 1 - fltValSetFrac - fltTestSetFrac\n",
    "\n",
    "blnShuffleIndices = argShuffleIndices\n",
    "\n",
    "print('fltValSetFrac = {}, fltTestSetFrac = {}, fltTrainSetFrac = {:.2f}'.format(fltValSetFrac, fltTestSetFrac, fltTrainSetFrac))\n",
    "print('blnShuffleIndices = {}'.format(blnShuffleIndices))\n",
    "print()\n",
    "\n",
    "# NOTE: We do not randomly shuffle the segment indices like in CNN trainings because\n",
    "#       data fed into RNNs are time-dependent, so we don't want the training and\n",
    "#       validation sets to be arranged out of sequence. However, we should still try\n",
    "#       a version where the model is trained by feeding the RNN with randomly arranged\n",
    "#       sequences to see if this indeed makes a difference\n",
    "lstTrainSegIndices = np.array(list(range(intTrainNumSegments)))  # Create a numpy array that goes from [0:intNumSegments]\n",
    "\n",
    "# Include the following code if we want to see what happens if we randomize the\n",
    "# training and validation sequences\n",
    "if (blnShuffleIndices):\n",
    "    np.random.seed(1)                      # Set random seed for reproducibility\n",
    "    np.random.shuffle(lstTrainSegIndices)  # Randomly shuffle the indices\n",
    "\n",
    "# The number of training indices is based on fltTrainSetFrac of the entire data set\n",
    "intTrainStartIdx = 0\n",
    "intTrainEndIdx   = round(intTrainNumSegments * fltTrainSetFrac)\n",
    "print('Training set [start:end] = [{}:{}]'.format(intTrainStartIdx, intTrainEndIdx))\n",
    "lstTrainIndices = lstTrainSegIndices[intTrainStartIdx:intTrainEndIdx]  # Get the list of training indices\n",
    "print('lstTrainIndices = {}'.format(lstTrainIndices))\n",
    "\n",
    "# The following fraction of the data set contributes to the validation set\n",
    "intValStartIdx = intTrainEndIdx\n",
    "intValEndIdx   = intTrainEndIdx + round(intTrainNumSegments * fltValSetFrac)\n",
    "print('Validation set [start:end] = [{}:{}]'.format(intValStartIdx, intValEndIdx))\n",
    "lstValIndices = lstTrainSegIndices[intValStartIdx:intValEndIdx]  # Get the list of validation indices\n",
    "print('lstValIndices = {}'.format(lstValIndices))\n",
    "\n",
    "# Extract the training and validation data from arrAllDataBatchFirst[]\n",
    "arrTrainingData = arrTrainingDataBatchFirst[lstTrainIndices, :, :]\n",
    "arrValData      = arrTrainingDataBatchFirst[lstValIndices, :, :]\n",
    "print('arrTrainingData.shape = {}, arrValData.shape = {}'.format(arrTrainingData.shape, arrValData.shape))\n",
    "\n",
    "# Split the segment types into training and validation labels as well\n",
    "arrTrainingLabels = arrTrainingSegTypes[lstTrainIndices]\n",
    "arrValLabels      = arrTrainingSegTypes[lstValIndices]\n",
    "print('arrTrainingLabels.shape = {}, arrValLabels.shape = {}'.format(arrTrainingLabels.shape, arrValLabels.shape))\n",
    "\n",
    "# Also create a test set if fltTestSetFrac > 0\n",
    "if (fltTestSetFrac > 0):\n",
    "    intTestStartIdx = intValEndIdx\n",
    "    intTestEndIdx = intTrainNumSegments\n",
    "    print()\n",
    "    print('Test set [start:end] = [{}:{}]'.format(intTestStartIdx, intTestEndIdx))\n",
    "    lstTestIndices = lstTrainSegIndices[intTestStartIdx:intTestEndIdx]\n",
    "    print('lstTestIndices = {}'.format(lstTestIndices))\n",
    "\n",
    "    arrTestDataFromTraining = arrTrainingDataBatchFirst[lstTestIndices, :, :]\n",
    "    print('arrTestDataFromTraining.shape = {}'.format(arrTestDataFromTraining.shape))\n",
    "\n",
    "    arrTestLabelsFromTraining = arrTrainingSegTypes[lstTestIndices]\n",
    "    print('arrTestLabelsFromTraining.shape = {}'.format(arrTestLabelsFromTraining.shape))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Visualize data after the split to confirm integrity\n",
    "intStartIdx = 0\n",
    "intEndIdx = 10\n",
    "#intEndIdx = len(lstTrainingSegLabels)\n",
    "\n",
    "print('Displaying a snapshot of the split data set (from subsequence {} to {}):\\n'.format(intStartIdx, intEndIdx))\n",
    "\n",
    "print('arrTrainingLabels[{}:{}] =\\n{}'.format(intStartIdx, intEndIdx, arrTrainingLabels[intStartIdx:intEndIdx]))\n",
    "print('arrValLabels[{}:{}] =\\n{}'.format(intStartIdx, intEndIdx, arrValLabels[intStartIdx:intEndIdx]))\n",
    "print('arrTestLabelsFromTraining[{}:{}] =\\n{}'.format(intStartIdx, intEndIdx, arrTestLabelsFromTraining[intStartIdx:intEndIdx]))\n",
    "print()\n",
    "\n",
    "print('Training set:')\n",
    "for tupZip in zip(lstTrainIndices[intStartIdx:intEndIdx],\n",
    "                  itemgetter(*lstTrainIndices[intStartIdx:intEndIdx])(lstTrainingSegLabels),\n",
    "                  arrTrainingLabels[intStartIdx:intEndIdx],\n",
    "                  itemgetter(*lstTrainIndices[intStartIdx:intEndIdx])(lstTrainingSequences),\n",
    "                  itemgetter(*lstTrainIndices[intStartIdx:intEndIdx])(lstTrainingSubSequences)):\n",
    "    print(*tupZip, sep = '\\t')\n",
    "print()\n",
    "\n",
    "print('Validation set:')\n",
    "for tupZip in zip(lstValIndices[intStartIdx:intEndIdx],\n",
    "                  itemgetter(*lstValIndices[intStartIdx:intEndIdx])(lstTrainingSegLabels),\n",
    "                  arrValLabels[intStartIdx:intEndIdx],\n",
    "                  itemgetter(*lstValIndices[intStartIdx:intEndIdx])(lstTrainingSequences), \n",
    "                  itemgetter(*lstValIndices[intStartIdx:intEndIdx])(lstTrainingSubSequences)):\n",
    "    print(*tupZip, sep = '\\t')\n",
    "print()\n",
    "\n",
    "print('Test set from training data:')\n",
    "for tupZip in zip(lstTestIndices[intStartIdx:intEndIdx],\n",
    "                  itemgetter(*lstTestIndices[intStartIdx:intEndIdx])(lstTrainingSegLabels),\n",
    "                  arrTestLabelsFromTraining[intStartIdx:intEndIdx],\n",
    "                  itemgetter(*lstTestIndices[intStartIdx:intEndIdx])(lstTrainingSequences), \n",
    "                  itemgetter(*lstTestIndices[intStartIdx:intEndIdx])(lstTrainingSubSequences)):\n",
    "    print(*tupZip, sep = '\\t')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training and test data/labels into DataLoader objects so we can easily iterate through\n",
    "# the data sets during training and testing\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "blnShuffleData = argShuffleData  # Shuffle data in DataLoader or not\n",
    "intBatchSize   = argBatchSize    # Set batch size for training\n",
    "\n",
    "print('blnShuffleData = {}'.format(blnShuffleData))\n",
    "print('intBatchSize = {}'.format(intBatchSize))\n",
    "print()\n",
    "\n",
    "# Convert training, validation, and test data and labels from np.arrays into data set wrapping tensors\n",
    "# for DataLoader\n",
    "objTrainDataset = TensorDataset(torch.from_numpy(arrTrainingData), torch.from_numpy(arrTrainingLabels))\n",
    "objValDataset   = TensorDataset(torch.from_numpy(arrValData), torch.from_numpy(arrValLabels))\n",
    "\n",
    "if (fltTestSetFrac > 0):\n",
    "    objTestDataset  = TensorDataset(torch.from_numpy(arrTestDataFromTraining), torch.from_numpy(arrTestLabelsFromTraining))\n",
    "    \n",
    "# Since this is an RNN we may not want to shuffle our data and lose some of the time-related history\n",
    "objTrainLoader = DataLoader(objTrainDataset, shuffle = blnShuffleData, batch_size = intBatchSize)\n",
    "objValLoader   = DataLoader(objValDataset, shuffle = blnShuffleData, batch_size = intBatchSize)\n",
    "\n",
    "if (fltTestSetFrac > 0):\n",
    "    objTestLoader  = DataLoader(objTestDataset, shuffle = blnShuffleData, batch_size = intBatchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Get one batch of training data and see how it looks\n",
    "iterTrainLoader = iter(objTrainLoader)  # Convert objTrainLoader into an iterator\n",
    "arrTrainDataBatch, arrTrainLabelsBatch = iterTrainLoader.next()\n",
    "\n",
    "print('arrTrainDataBatch.shape = {}, arrTrainLabelsBatch = {}'.format(arrTrainDataBatch.shape, arrTrainLabelsBatch.shape))\n",
    "print('arrTrainLabelsBatch = {}'.format(arrTrainLabelsBatch))\n",
    "\n",
    "# Check that the data contained in arrTrainingDataBatchFirst[] and arrTrainDataBatch[]\n",
    "# are still consistent (data will not match if blnShuffleData = True)\n",
    "print('arrTrainingDataBatchFirst = \\n{}'.format(arrTrainingDataBatchFirst[0:3, 0:10, 0]))\n",
    "print('arrTrainDataBatch = \\n{}'.format(arrTrainDataBatch[0:3, 0:10, 0]))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available and if so, set a device to use\n",
    "\n",
    "intGPUDevice = argGPUDevice\n",
    "\n",
    "blnTrainOnGPU = torch.cuda.is_available()\n",
    "\n",
    "if(blnTrainOnGPU):\n",
    "    intNumGPUs = torch.cuda.device_count()\n",
    "    print('Training on GPU ({} available):'.format(intNumGPUs))\n",
    "    for intGPU in range(intNumGPUs):\n",
    "        print('  Device {}: {}'.format(intGPU, torch.cuda.get_device_name(intGPU)))\n",
    "    torch.cuda.set_device(intGPUDevice)\n",
    "    print('Using GPU #{}'.format(intGPUDevice))\n",
    "else:\n",
    "    print('No GPU available, training on CPU')\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "intFeaturesDim = intTrainNumChannels  # TODO: Need to mnake this customizable to use a subset of channels\n",
    "intHiddenDim   = argHiddenDim\n",
    "intNumLayers   = argNumLayers\n",
    "intOutputSize  = argOutputSize        # TODO: Extract this from training data (2 states: interictal and preictal)\n",
    "fltDropProb    = argDropProb\n",
    "\n",
    "print('intFeaturesDim = {}, intHiddenDim = {}, intNumLayers = {}, intOutputSize = {}, fltDropProb = {}'.format(intFeaturesDim, intHiddenDim, intNumLayers, intOutputSize, fltDropProb))\n",
    "print()\n",
    "\n",
    "objModelLSTM = LSTM.clsLSTM(intFeaturesDim, intHiddenDim, intNumLayers, intOutputSize, argDropProb = fltDropProb)\n",
    "print(objModelLSTM)\n",
    "print()\n",
    "\n",
    "objModelLSTM.showParams()\n",
    "print()\n",
    "\n",
    "# Print the total number of parameters in the model\n",
    "intTotalParams = sum(objParam.numel() for objParam in objModelLSTM.parameters() if objParam.requires_grad)\n",
    "print('intTotalParams = {} ({})'.format(intTotalParams, type(intTotalParams)))\n",
    "print()\n",
    "\n",
    "# Print the size of the dataset\n",
    "print('Dataset size = intTrainNumSegments * intTrainSeqLen * intTrainNumChannels = {} x {} x {} = {} ({})'.format(intTrainNumSegments, intTrainSeqLen, intTrainNumChannels, intTrainNumSegments * intTrainSeqLen * intTrainNumChannels, arrTrainingData.dtype))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss criterion and optimization function\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "fltLearningRate = argLearningRate\n",
    "\n",
    "# If argClassWeights[] is empty, assign equal weight to each class\n",
    "if (not argClassWeights):\n",
    "    argClassWeights = [1] * argOutputSize  # Assign equal weight to each class\n",
    "\n",
    "argClassWeights = np.array(argClassWeights, dtype = np.float32)\n",
    "print('argClassWeights = {}'.format(argClassWeights))\n",
    "\n",
    "arrClassWeights = torch.tensor(argClassWeights).cuda()\n",
    "objCriterion = nn.CrossEntropyLoss(weight = arrClassWeights)\n",
    "\n",
    "if (argOptimizer == 0):\n",
    "    strOptimizer = 'Adam'\n",
    "    objOptimizer = torch.optim.Adam(objModelLSTM.parameters(), lr = fltLearningRate)\n",
    "elif (argOptimizer == 1):\n",
    "    strOptimizer = 'AdamW'\n",
    "    objOptimizer = torch.optim.AdamW(objModelLSTM.parameters(), lr = fltLearningRate)\n",
    "elif (argOptimizer == 2):\n",
    "    strOptimizer = 'SGD'\n",
    "    objOptimizer = torch.optim.SGD(objModelLSTM.parameters(), lr = fltLearningRate)\n",
    "else:\n",
    "    strOptimizer = 'Adam'\n",
    "    objOptimizer = torch.optim.Adam(objModelLSTM.parameters(), lr = fltLearningRate)\n",
    "    \n",
    "print('strOptimizer = {}, fltLearningRate = {}'.format(strOptimizer, fltLearningRate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "\n",
    "# Define training parameters\n",
    "blnDebug = False\n",
    "\n",
    "intNumEpochs   = argNumEpochs    # Number of epochs (entire training set) to train the model\n",
    "intValPerEpoch = argValPerEpoch  # Number of validation loops per epoch\n",
    "fltGradClip    = argGradClip     # Value at which gradient is clipped\n",
    "\n",
    "# Calculate validation loss every n training batches/steps\n",
    "intNumBatchLoops = int(round(arrTrainingData.shape[0] / intBatchSize))\n",
    "intPrintEvery = intNumBatchLoops // intValPerEpoch\n",
    "\n",
    "print('intNumEpochs = {}, intValPerEpoch = {}, intPrintEvery = {}, fltGradClip = {}'.format(intNumEpochs, intValPerEpoch, intPrintEvery, fltGradClip))\n",
    "print()\n",
    "\n",
    "intBatchLoopIdx = 0  # Loop index for training model with batches of data\n",
    "lstTrainingStepLosses, lstValidationStepLosses = [], []  # List of losses every n training batches/steps\n",
    "\n",
    "# Move the model to the GPU if one is available\n",
    "if(blnTrainOnGPU):\n",
    "    objModelLSTM.cuda()\n",
    "\n",
    "objModelLSTM.train()\n",
    "\n",
    "datTrainingStart = utils.fnNow()\n",
    "print('Training started on {}'.format(utils.fnGetDatetime(datTrainingStart)))\n",
    "\n",
    "# Train for a specified number of epochs\n",
    "for intEpoch in range(intNumEpochs):\n",
    "    print('intEpoch = {}'.format(intEpoch + 1))\n",
    "    \n",
    "    # Initialize hidden and cell states\n",
    "    arrHiddenState = objModelLSTM.initHidden(intBatchSize, blnTrainOnGPU, argDebug = False)  # Batch size defined above when creating DataLoader\n",
    "\n",
    "    # Batch loop (each loop trains one batch of input data)\n",
    "    for arrInputData, arrLabels in objTrainLoader:\n",
    "        print('  intBatchLoopIdx = {}.{} (batch size = {})'.format(intEpoch + 1, intBatchLoopIdx + 1, arrInputData.shape[0]))\n",
    "        intBatchLoopIdx += 1  # New batch/training loop\n",
    "        \n",
    "        # If batch size allocated from DataLoader is smaller than intBatchSize\n",
    "        # (which happens on the last batch when the data set is not divisible\n",
    "        # by intBatchSize), break out of the loop\n",
    "        \n",
    "        # TODO: This is the strategy for now until we figure out what the best\n",
    "        #       strategy is on how/whether to initialize the hidden state with\n",
    "        #       a smaller batch size for the last orphan batch\n",
    "        if (intBatchSize != arrInputData.shape[0]):\n",
    "            print('    Exiting training loop (intBatchSize = {}, arrInputData.shape[0] = {})'.format(intBatchSize, arrInputData.shape[0]))\n",
    "            break\n",
    "        \n",
    "        if(blnTrainOnGPU):\n",
    "            arrInputData, arrLabels = arrInputData.cuda(), arrLabels.cuda()\n",
    "\n",
    "        # Extract new variables for the hidden and cell states to decouple states\n",
    "        # from backprop history. Otherwise the gradient will be backpropagated\n",
    "        # through the entire training history\n",
    "        arrHiddenState = tuple([arrState.data for arrState in arrHiddenState])  # Getting only the data portion\n",
    "                                                                                # of the hidden/cell states detaches\n",
    "                                                                                # them from the backprop history\n",
    "\n",
    "        # Zero the accumulated gradients\n",
    "        objModelLSTM.zero_grad()\n",
    "\n",
    "        if (blnDebug):\n",
    "            print('    arrInputData.shape = {}, arrInputData.type() = {}'.format(arrInputData.shape, arrInputData.type()))\n",
    "            print('    arrLabels.shape = {}, arrLabels.type() = {}'.format(arrLabels.shape, arrLabels.type()))\n",
    "            print('    arrHiddenState.shape = ({}, {}), arrHiddenState.type() = ({}, {})'.format(arrHiddenState[0].shape, arrHiddenState[1].shape, arrHiddenState[0].type(), arrHiddenState[1].type()))\n",
    "            print('    arrLabels = {}'.format(arrLabels))\n",
    "            \n",
    "        # Forward pass through the model and get the next hidden state and output\n",
    "        # Output shape = (batch_size, 1), h shape = (n_layers, batch_size, hidden_dim)\n",
    "        arrOutput, arrHiddenState = objModelLSTM.forward(arrInputData, arrHiddenState, argDebug = False)\n",
    "\n",
    "        if (blnDebug):\n",
    "            print('    arrOutput.shape = {}, arrOutput.type() = {}'.format(arrOutput.shape, arrOutput.type()))\n",
    "            print('    arrHiddenState.shape = ({}, {}), arrHiddenState.type() = ({}, {})'.format(arrHiddenState[0].shape, arrHiddenState[1].shape, arrHiddenState[0].type(), arrHiddenState[1].type()))\n",
    "            print('    arrOutput = \\n{}'.format(arrOutput))\n",
    "        \n",
    "        # Calculate the loss and perform backprop (looks like output shape is\n",
    "        # not changed after the squeeze)\n",
    "        # NOTE: arrOutput[] is returned as float since the values are close to\n",
    "        #       (but not exactly) 0 or 1. However, arrLabels[] is expected to\n",
    "        #       be of type long)\n",
    "        fltTrainingLoss = objCriterion(arrOutput, arrLabels)  # arrOutput[] = float, arrLabels[] = long\n",
    "        print('    fltTrainingLoss = {:.6f} ({})'.format(fltTrainingLoss, fltTrainingLoss.type()))\n",
    "        \n",
    "        fltTrainingLoss.backward()\n",
    "        if (blnDebug):\n",
    "            print('    arrOutput.squeeze().shape = {}'.format(arrOutput.squeeze().shape))\n",
    "        \n",
    "        # Using clip_grad_norm() helps prevent the exploding gradient\n",
    "        # problem in RNNs / LSTMs\n",
    "        nn.utils.clip_grad_norm_(objModelLSTM.parameters(), fltGradClip)\n",
    "        objOptimizer.step()\n",
    "\n",
    "        # Calculate loss statistics\n",
    "        if (intBatchLoopIdx % intPrintEvery == 0):\n",
    "            print('  Calculating loss statistics...')\n",
    "            \n",
    "            # Get validation loss\n",
    "            arrValHiddenState = objModelLSTM.initHidden(intBatchSize, blnTrainOnGPU, argDebug = False)\n",
    "            lstValidationBatchLosses = []\n",
    "            \n",
    "            objModelLSTM.eval()\n",
    "            \n",
    "            for arrInputData, arrLabels in objValLoader:\n",
    "                print('    In batch loop... (batch size = {})'.format(arrInputData.shape[0]))\n",
    "                \n",
    "                # If batch size allocated from DataLoader is smaller than intBatchSize\n",
    "                # (which happens on the last batch when the data set is not divisible\n",
    "                # by intBatchSize), break out of the loop\n",
    "\n",
    "                # TODO: This is the strategy for now until we figure out what the best\n",
    "                #       strategy is on how/whether to initialize the hidden state with\n",
    "                #       a smaller batch size for the last orphan batch\n",
    "                if (intBatchSize != arrInputData.shape[0]):\n",
    "                    print('    Exiting validation loop (intBatchSize = {}, arrInputData.shape[0] = {})'.format(intBatchSize, arrInputData.shape[0]))\n",
    "                    break\n",
    "                \n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                arrValHiddenState = tuple([arrState.data for arrState in arrValHiddenState])\n",
    "\n",
    "                if(blnTrainOnGPU):\n",
    "                    arrInputData, arrLabels = arrInputData.cuda(), arrLabels.cuda()\n",
    "\n",
    "                arrOutput, arrValHiddenState = objModelLSTM.forward(arrInputData, arrValHiddenState)\n",
    "                \n",
    "                fltValidationLoss = objCriterion(arrOutput, arrLabels)\n",
    "                print('    fltValidationLoss = {:.6f} ({})'.format(fltValidationLoss, fltValidationLoss.type()))\n",
    "\n",
    "                lstValidationBatchLosses.append(fltValidationLoss.item())\n",
    "\n",
    "            objModelLSTM.train()\n",
    "            \n",
    "            # Clear the GPU cache regularly to avoid the following CUDA error:\n",
    "            #\n",
    "            #   RuntimeError: CUDA out of memory. Tried to allocate 6.61 GiB \n",
    "            #   (GPU 1; 10.76 GiB total capacity; 1.25 GiB already allocated; \n",
    "            #   2.39 GiB free; 6.38 GiB cached)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            print('  Epoch: {}/{}...'.format(intEpoch + 1, intNumEpochs),\n",
    "                  'Step: {}...'.format(intBatchLoopIdx),\n",
    "                  'Training Loss: {:.6f}...'.format(fltTrainingLoss.item()),\n",
    "                  'Validation Loss: {:.6f}'.format(np.mean(lstValidationBatchLosses)))\n",
    "            \n",
    "            lstTrainingStepLosses.append(fltTrainingLoss.item())\n",
    "            lstValidationStepLosses.append(np.mean(lstValidationBatchLosses))\n",
    "            \n",
    "print()\n",
    "\n",
    "datTrainingEnd = utils.fnNow()\n",
    "print('Training ended on {}'.format(utils.fnGetDatetime(datTrainingEnd)))\n",
    "\n",
    "datTrainingDuration = datTrainingEnd - datTrainingStart\n",
    "print('datTrainingDuration = {}'.format(datTrainingDuration))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and validation loss for the entire training\n",
    "if (not blnBatchMode):\n",
    "    utils.fnPlotTrainValLosses(lstTrainingStepLosses, lstValidationStepLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to the file system\n",
    "\n",
    "strModelDir = './SavedModels/'  # TODO: Make this into an argument?\n",
    "strDataSetName, strCSVName = dio.fnGetDataSetInfo(argCSVPath)\n",
    "\n",
    "strModelName = 'EEGLSTM_' + strDataSetName + \\\n",
    "               '_' + strCSVName + \\\n",
    "               '_Epoch-' + str(intNumEpochs) + \\\n",
    "               '_TLoss-' + str('{0:.4f}'.format(lstTrainingStepLosses[-1])) + \\\n",
    "               '_VLoss-' + str('{0:.4f}'.format(lstValidationStepLosses[-1])) + \\\n",
    "               '_' + strTimestamp + \\\n",
    "               '.net'\n",
    "\n",
    "# Create a new directory if it does not exist\n",
    "utils.fnOSMakeDir(strModelDir)\n",
    "\n",
    "# Collect all training and model-specific variables and passing them into the model as\n",
    "# variable named arguments (**kwargs). This is to make the model more self-contained, and\n",
    "# in this way there is no need to keep changing fnSaveLSTMModel() as we decide to include\n",
    "# more information in the saved model\n",
    "dctModelProperties = {\n",
    "    'strDataSetName':       strDataSetName,\n",
    "    'strCSVName':           strCSVName,\n",
    "    'strModelName':         strModelName,\n",
    "    \n",
    "    'lstTrainingFiles':     sorted(set(lstTrainingFilenames)),\n",
    "    'lstTrainingChannels':  lstTrainingChannels[0],  # TODO: Assumes that the list of channels are identical across all data subsequences\n",
    "    \n",
    "    'fltResamplingFreq':    argResamplingFreq,\n",
    "    'fltSubSeqDuration':    argSubSeqDuration,\n",
    "    'intStepSizeTimePts':   argStepSizeTimePts,\n",
    "    'dctStepSizeStates':    argStepSizeStates,\n",
    "    'fltSubWindowFraction': argSubWindowFraction,\n",
    "    \n",
    "    'intScalingMode':       argScalingMode,\n",
    "    'fltScaledMin':         argScaledMin,\n",
    "    'fltScaledMax':         argScaledMax,\n",
    "    'tupScalingInfo':       tupScalingInfo,\n",
    "    \n",
    "    'intImbalFactor':       intImbalFactor,\n",
    "    \n",
    "    'fltValSetFrac':        argValSetFrac,\n",
    "    'fltTestSetFrac':       argTestSetFrac,\n",
    "    'blnShuffleIndices':    argShuffleIndices,       # Already passed in as positional argument (not removed for backwards-compatibility)\n",
    "    'blnShuffleData':       argShuffleData,          # Already passed in as positional argument (not removed for backwards-compatibility)\n",
    "    'intBatchSize':         argBatchSize,            # Already passed in as positional argument (not removed for backwards-compatibility)\n",
    "    'arrClassWeights':      argClassWeights,\n",
    "    'intOptimizer':         argOptimizer,\n",
    "    'fltLearningRate':      argLearningRate,         # Already passed in as positional argument (not removed for backwards-compatibility)\n",
    "    'intNumEpochs':         argNumEpochs,            # Already passed in as positional argument (not removed for backwards-compatibility)\n",
    "    'intValPerEpoch':       argValPerEpoch,\n",
    "    'intPrintEvery':        intPrintEvery,           # Already passed in as positional argument (not removed for backwards-compatibility)\n",
    "    'fltGradClip':          argGradClip              # Already passed in as positional argument (not removed for backwards-compatibility)\n",
    "}\n",
    "\n",
    "if (blnBatchMode):\n",
    "    dctModelProperties['strLogFilename'] = strLogFilename  # Record log file only if it exists (when trained in batch mode)\n",
    "\n",
    "LSTM.fnSaveLSTMModel(strModelDir, strModelName, intTrainNumChannels, intTrainSeqLen, intTrainNumSegments, \n",
    "                     objModelLSTM, intNumEpochs, intBatchSize, blnShuffleIndices, blnShuffleData, fltLearningRate, intPrintEvery, fltGradClip, \n",
    "                     lstTrainingStepLosses, lstValidationStepLosses, **dctModelProperties)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model with test data set and record test losses & prediction accuracy\n",
    "\n",
    "blnDebug = True\n",
    "lstTestLosses = []  # Record test losses per batch/step\n",
    "intNumCorrect = 0   # Number of correctly predicted sequences in a batch size of (intBatchSize)\n",
    "\n",
    "# Initialize hidden and cell states\n",
    "arrHiddenState = objModelLSTM.initHidden(intBatchSize, blnTrainOnGPU, argDebug = False)  # Batch size defined above when creating DataLoader\n",
    "\n",
    "# Move the model to the GPU if one is available\n",
    "if(blnTrainOnGPU):\n",
    "    objModelLSTM.cuda()\n",
    "\n",
    "objModelLSTM.eval()\n",
    "\n",
    "#objTestLoader = objLabeledTestLoader\n",
    "\n",
    "# Batch loop (each loop trains one batch of input data)\n",
    "for arrInputData, arrLabels in objTestLoader:\n",
    "    print('Feed forwarding new test batch...')\n",
    "    \n",
    "    # If batch size allocated from DataLoader is smaller than intBatchSize\n",
    "    # (which happens on the last batch when the data set is not divisible\n",
    "    # by intBatchSize), break out of the loop\n",
    "\n",
    "    # TODO: This is the strategy for now until we figure out what the best\n",
    "    #       strategy is on how/whether to initialize the hidden state with\n",
    "    #       a smaller batch size for the last orphan batch\n",
    "    if (intBatchSize != arrInputData.shape[0]):\n",
    "        print('Exiting test loop (intBatchSize = {}, arrInputData.shape[0] = {})'.format(intBatchSize, arrInputData.shape[0]))\n",
    "        break\n",
    "                \n",
    "    if(blnTrainOnGPU):\n",
    "        arrInputData, arrLabels = arrInputData.cuda(), arrLabels.cuda()\n",
    "\n",
    "    # Extract new variables for the hidden and cell states to decouple states\n",
    "    # from backprop history. Otherwise the gradient will be backpropagated\n",
    "    # through the entire training history\n",
    "    arrHiddenState = tuple([arrState.data for arrState in arrHiddenState])\n",
    "\n",
    "    # Forward pass through the model and get the next hidden state and output\n",
    "    # Output shape = (batch_size, 1), h shape = (n_layers, batch_size, hidden_dim)\n",
    "    arrOutput, arrHiddenState = objModelLSTM.forward(arrInputData, arrHiddenState)\n",
    "\n",
    "    if (blnDebug):\n",
    "        #print('  arrLabels.shape = {}, arrLabels.type() = {}'.format(arrLabels.shape, arrLabels.type()))\n",
    "        #print('  arrOutput.shape = {}, arrOutput.type() = {}'.format(arrOutput.shape, arrOutput.type()))\n",
    "        print('  arrLabels = {}'.format(arrLabels))\n",
    "        print('  arrOutput = \\n{}'.format(arrOutput))\n",
    "\n",
    "    # Calculate test loss for this batch\n",
    "    fltTestLoss = objCriterion(arrOutput, arrLabels)\n",
    "    print('  fltTestLoss = {:.6f} ({})'.format(fltTestLoss, fltTestLoss.type()))\n",
    "\n",
    "    # Record test loss for this batch\n",
    "    lstTestLosses.append(fltTestLoss.item())\n",
    "\n",
    "    # Convert output scores between classes to one-hot encoding\n",
    "    # that indicate the predictions for each batch\n",
    "    _, arrPredictions = torch.max(arrOutput, 1)\n",
    "\n",
    "    # Compare the class predictions to the test set labels\n",
    "    arrCorrect = arrPredictions.eq(arrLabels)\n",
    "    if (blnTrainOnGPU):\n",
    "        arrCorrect = arrCorrect.cpu().numpy()  # Convert tensor back to np.array\n",
    "    else:\n",
    "        arrCorrect = arrCorrect.numpy()  # Convert tensor back to np.array\n",
    "    intNumCorrect += np.sum(arrCorrect)  # Count the number of correctly predicted sequences\n",
    "\n",
    "    # Clear the GPU cache regularly to avoid the following CUDA error:\n",
    "    #\n",
    "    #   RuntimeError: CUDA out of memory. Tried to allocate 6.61 GiB \n",
    "    #   (GPU 1; 10.76 GiB total capacity; 1.25 GiB already allocated; \n",
    "    #   2.39 GiB free; 6.38 GiB cached)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Print the mean test loss for the entire test set\n",
    "print(\"Test Loss = {:.4f}\".format(np.mean(lstTestLosses)))\n",
    "\n",
    "# Print the test accuracy over all test data\n",
    "fltTestAccuracy = intNumCorrect / len(objTestLoader.dataset)\n",
    "print(\"Test Accuracy = {}/{} = {:.4f}\".format(int(round(intNumCorrect)), len(objTestLoader.dataset), fltTestAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the log file and redirect output back to stdout and stderr\n",
    "if (blnBatchMode):\n",
    "    \n",
    "    datScriptEnd = utils.fnNow()\n",
    "    print('Script ended on {}'.format(utils.fnGetDatetime(datScriptEnd)))\n",
    "\n",
    "    datScriptDuration = datScriptEnd - datScriptStart\n",
    "    print('datScriptDuration = {}'.format(datScriptDuration))\n",
    "\n",
    "    objLogFile.close()\n",
    "    sys.stdout = objStdout\n",
    "    sys.stderr = objStderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send email notification to the specified email address(es) to notify\n",
    "# the end of the script if in batch mode\n",
    "if (blnBatchMode):\n",
    "    \n",
    "    strEmailLogin  = argEmailLogin\n",
    "    strEmailPasswd = argEmailPasswd\n",
    "    strFromEmail   = argFromEmail\n",
    "    strToEmails    = argToEmails\n",
    "\n",
    "    print('Sending notification to:')\n",
    "    print('  strEmailLogin = {}'.format(strEmailLogin))\n",
    "    print('  strFromEmail = {}'.format(strFromEmail))\n",
    "    print('  strToEmails = {}'.format(strToEmails))\n",
    "\n",
    "    strSubject = 'Script execution ended (scrTrainLSTM.py)'\n",
    "    strBody = 'scrTrainLSTM.py finished execution\\nDuration = {}\\nLog file = {}'.format(datScriptDuration, strLogFilename)\n",
    "\n",
    "    utils.fnSendMail(strGmailSMTPServer, intGmailSMTPPort, strEmailLogin, strEmailPasswd, strFromEmail, strToEmails, strSubject, strBody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
