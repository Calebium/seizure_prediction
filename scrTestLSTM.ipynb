{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and functions for this script\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import libDataIO as dio\n",
    "import libModelLSTM as LSTM\n",
    "import libUtils as utils\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter magic commands that should only be run when the code is running\n",
    "# in Jupyter. Set to blnBatchMode to True when running in batch mode \n",
    "blnBatchMode = utils.fnIsBatchMode()\n",
    "\n",
    "if (blnBatchMode):\n",
    "    print('Running in BATCH mode...')\n",
    "    \n",
    "else:\n",
    "    print('Running in INTERACTIVE mode...')\n",
    "    # Automatically reload modules before code execution\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    # Set plotting style\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up script and training parameters from command line arguments (batch mode)\n",
    "# or hard-coded values in the script\n",
    "if (blnBatchMode):\n",
    "    \n",
    "    # BATCH MODE ONLY: This cell will execute in batch mode and parse the relevant\n",
    "    #                  command line arguments\n",
    "    \n",
    "    import argparse\n",
    "\n",
    "    # Construct argument parser\n",
    "    objArgParse = argparse.ArgumentParser()\n",
    "\n",
    "    # Add arguments to the parser\n",
    "    objArgParse.add_argument('-md',   '--modeldir',          required = True,                   help = '')\n",
    "    objArgParse.add_argument('-mn',   '--modelname',         required = True,                   help = '')\n",
    "    objArgParse.add_argument('-tcsv', '--csvpath',           required = True,                   help = '')\n",
    "    \n",
    "    objArgParse.add_argument('-rf',   '--resamplingfreq',    required = False, default = -1,    help = '')  # Will be over-written from model\n",
    "    objArgParse.add_argument('-du',   '--subseqduration',    required = False, default = -1,    help = '')  # Will be over-written from model\n",
    "    objArgParse.add_argument('-ss',   '--stepsizetimepts',   required = False, default = -1,    help = '')  # Will be over-written from model\n",
    "    objArgParse.add_argument('-sss',  '--stepsizestates', type = json.loads, required = False, default = '{}', help = '')  # Example use: -sss '{\"ictal\": 128}'\n",
    "    objArgParse.add_argument('-sw',   '--subwindowfraction', required = False, default = -1,    help = '')  # Will be over-written from model\n",
    "    \n",
    "    objArgParse.add_argument('-smod', '--scalingmode',       required = False, default = -1,    help = '')  # Will be over-written from model\n",
    "    objArgParse.add_argument('-smin', '--scaledmin',         required = False, default = -1,    help = '')  # Will be over-written from model\n",
    "    objArgParse.add_argument('-smax', '--scaledmax',         required = False, default = 1,     help = '')  # Will be over-written from model\n",
    "    \n",
    "    objArgParse.add_argument('-sd',   '--shuffledata',       required = False, default = False, help = '')  # No real need to shuffle for testing\n",
    "    \n",
    "    objArgParse.add_argument('-gpu',  '--gpudevice',         required = False, default = -1,    help = '')\n",
    "    objArgParse.add_argument('-sa',   '--saveanno',          required = False, default = False, help = '')\n",
    "\n",
    "    dctArgs = vars(objArgParse.parse_args())\n",
    "\n",
    "    # Convert parameters extract from arguments to their appropriate date types\n",
    "    argModelDir          = dctArgs['modeldir']\n",
    "    argModelName         = dctArgs['modelname']\n",
    "    argCSVPath           = dctArgs['csvpath']\n",
    "    \n",
    "    argResamplingFreq    = int(dctArgs['resamplingfreq'])\n",
    "    argSubSeqDuration    = int(dctArgs['subseqduration'])\n",
    "    argStepSizeTimePts   = int(dctArgs['stepsizetimepts'])\n",
    "    argStepSizeStates    = dctArgs['stepsizestates']\n",
    "    argStepSizeStates    = {}     # Step size of sliding window for specific segment states (default = {}, use -ssv value)\n",
    "    argSubWindowFraction = float(dctArgs['subwindowfraction'])\n",
    "    \n",
    "    argScalingMode       = int(dctArgs['scalingmode'])\n",
    "    argScaledMin         = int(dctArgs['scaledmin'])\n",
    "    argScaledMax         = int(dctArgs['scaledmax'])\n",
    "    \n",
    "    argScalingParams = () if (argScalingMode == -1) else (argScalingMode, (argScaledMin, argScaledMax))\n",
    "    \n",
    "    argShuffleData       = dctArgs['shuffledata']\n",
    "    argGPUDevice         = int(dctArgs['gpudevice'])\n",
    "    argSaveAnno          = dctArgs['saveanno']\n",
    "    \n",
    "else:\n",
    "    # Set all configurable parameters of the script as arguments. After\n",
    "    # these parameters are set, the entire script can be run in one go\n",
    "\n",
    "    # Specify the path where the trained models are saved\n",
    "    argModelDir = './SavedModels/'\n",
    "\n",
    "    # Specify the model name to use for testing\n",
    "    argModelName = 'EEGLSTM_CHB-MIT_chb15_Epoch-1_TLoss-0.5327_VLoss-0.6197_20200617-044737.net'  # chb15 (AdamW, smod = 0), epoch = 1 -> hard to tell if there is convergence -> test accuracy from training = 0.82 although test loss = 0.5464. Almost zero ictal prediction, good interictal prediction\n",
    "    \n",
    "    # Specify the CSV file that lists the EEG segments to use for testing\n",
    "    argCSVPath        = './DataCSVs/CHB-MIT/chb15_Test.csv'\n",
    "\n",
    "    # Specify the resampling frequency and subsequence durations for the\n",
    "    # EEG segments of the test set (most of the following arguments will\n",
    "    # be over-written by those from the loaded model if the model contains\n",
    "    # dctModelProperties{})\n",
    "    argResamplingFreq    = -1\n",
    "    argSubSeqDuration    = 1\n",
    "    argStepSizeTimePts   = -1\n",
    "    argStepSizeStates    = {}     # Step size of sliding window for specific segment states (default = {}, use -ssv value)\n",
    "    argSubWindowFraction = 0.3\n",
    "    \n",
    "    argScalingMode       = 1\n",
    "    argScaledMin         = -1\n",
    "    argScaledMax         = 1\n",
    "    \n",
    "    argScalingParams = () if (argScalingMode == -1) else (argScalingMode, (argScaledMin, argScaledMax))\n",
    "    \n",
    "    # There is really no need to shuffle data when we're feeding in the test set\n",
    "    argShuffleData       = True  # Shuffle data in DataLoader or not (should not affect test results)\n",
    "\n",
    "    # Specify which GPU device to use\n",
    "    argGPUDevice = 0\n",
    "    \n",
    "    # Specify whether to save test results to annotation files\n",
    "    argSaveAnno = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a timestamp that is unique to this run\n",
    "strTimestamp = str(utils.fnGenTimestamp())\n",
    "print('strTimestamp = {}'.format(strTimestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a log file only when in batch mode\n",
    "if (blnBatchMode):\n",
    "    # Log all output messages to a log file when it is in Batch mode\n",
    "    strLogDir = './Logs/'  # TODO: Make this into an argument?\n",
    "\n",
    "    # Create a new directory if it does not exist\n",
    "    utils.fnOSMakeDir(strLogDir)\n",
    "\n",
    "    # Saving the original stdout and stderr\n",
    "    objStdout = sys.stdout\n",
    "    objStderr = sys.stderr\n",
    "\n",
    "    strLogFilename = 'runTestLSTM_' + strTimestamp + '_' + argModelName + '.log'\n",
    "    print('strLogFilename = {}'.format(strLogFilename))\n",
    "\n",
    "    # Open a new log file\n",
    "    objLogFile = open(strLogDir + strLogFilename, 'w')\n",
    "\n",
    "    # Replace stdout and stderr with log file so all print statements will\n",
    "    # be redirected to the log file from this point on\n",
    "    sys.stdout = objLogFile\n",
    "    sys.stderr = objLogFile\n",
    "\n",
    "    datScriptStart = utils.fnNow()\n",
    "    print('Script started on {}'.format(utils.fnGetDatetime(datScriptStart)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved LSTM model from the file system\n",
    "\n",
    "strModelDir = argModelDir\n",
    "strModelName = argModelName\n",
    "\n",
    "(intTrainNumChannels, intTrainSeqLen, intTrainNumSegments, objModelLSTM,\n",
    " intNumEpochs, intBatchSize, blnShuffleIndices, blnShuffleData, fltLearningRate, intPrintEvery, fltGradClip,\n",
    " lstTrainingStepLosses, lstValidationStepLosses, dctModelProperties) = LSTM.fnLoadLSTMModel(strModelDir, strModelName)\n",
    "\n",
    "# Over-write arguments from the script with values saved in the model\n",
    "if (dctModelProperties):\n",
    "    lstTrainingChannels  = dctModelProperties['lstTrainingChannels']\n",
    "    \n",
    "    argResamplingFreq    = dctModelProperties['fltResamplingFreq']\n",
    "    argSubSeqDuration    = dctModelProperties['fltSubSeqDuration']\n",
    "    argStepSizeTimePts   = dctModelProperties['intStepSizeTimePts']\n",
    "    argStepSizeStates    = utils.fnFindInDct(dctModelProperties, 'dctStepSizeStates', argStepSizeStates)  # May not exist in some models\n",
    "    argSubWindowFraction = dctModelProperties['fltSubWindowFraction']\n",
    "\n",
    "    argScalingMode       = dctModelProperties['intScalingMode']\n",
    "    argScaledMin         = dctModelProperties['fltScaledMin']\n",
    "    argScaledMax         = dctModelProperties['fltScaledMax']\n",
    "\n",
    "    argScalingParams = () if (argScalingMode == -1) else (argScalingMode, (argScaledMin, argScaledMax))\n",
    "    \n",
    "    tupScalingInfo       = utils.fnFindInDct(dctModelProperties, 'tupScalingInfo', ())  # May not exist in some models\n",
    "    \n",
    "    intValPerEpoch       = dctModelProperties['intValPerEpoch']\n",
    "    \n",
    "    print('\\n  dctModelProperties{} exists in saved model. Over-writting the following arguments with values saved in the model:')\n",
    "    print()\n",
    "    print('    argResamplingFreq = {}'.format(argResamplingFreq))\n",
    "    print('    argSubSeqDuration = {}'.format(argSubSeqDuration))\n",
    "    print('    argStepSizeTimePts = {}'.format(argStepSizeTimePts))\n",
    "    print('    argStepSizeStates = {}'.format(argStepSizeStates))\n",
    "    print('    argSubWindowFraction = {}'.format(argSubWindowFraction))\n",
    "    print('    argScalingParams = {}'.format(argScalingParams))\n",
    "    print()\n",
    "    \n",
    "    if ('strLogFilename' in dctModelProperties.keys()):\n",
    "        print('    strLogFilename = {}'.format(dctModelProperties['strLogFilename']))\n",
    "        print()\n",
    "    \n",
    "else:\n",
    "    lstTrainingChaneels = []\n",
    "    tupScalingInfo      = ()\n",
    "    intValPerEpoch      = -1\n",
    "    \n",
    "    print('\\n  WARNING: dctModelProperties{} not found in saved model. Using arguments specified in this training script')\n",
    "    print()\n",
    "\n",
    "print('lstTrainingChannels = {}'.format(lstTrainingChannels))\n",
    "print('len(tupScalingInfo) = {}'.format(len(tupScalingInfo)))\n",
    "print('intValPerEpoch = {}'.format(intValPerEpoch))\n",
    "print()\n",
    "\n",
    "utils.fnShowMemUsage()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out all specified arguments\n",
    "print('argModelDir = {}'.format(argModelDir))\n",
    "print('argModelName = {}'.format(argModelName))\n",
    "print('argCSVPath = {}'.format(argCSVPath))\n",
    "\n",
    "print('argResamplingFreq = {}'.format(argResamplingFreq))\n",
    "print('argSubSeqDuration = {}'.format(argSubSeqDuration))\n",
    "print('argStepSizeTimePts = {}'.format(argStepSizeTimePts))\n",
    "print('argStepSizeStates = {}'.format(argStepSizeStates))\n",
    "print('argSubWindowFraction = {}'.format(argSubWindowFraction))\n",
    "\n",
    "print('argScalingParams = {}'.format(argScalingParams))\n",
    "\n",
    "if (tupScalingInfo):\n",
    "    print('  -> Scaling across training and test files')\n",
    "\n",
    "print('argShuffleData = {}'.format(argShuffleData))\n",
    "\n",
    "print('argGPUDevice = {}'.format(argGPUDevice))\n",
    "\n",
    "print('argSaveAnno = {}'.format(argSaveAnno))\n",
    "\n",
    "print()\n",
    "\n",
    "utils.fnShowMemUsage()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and validation loss for the entire training if in interactive mode\n",
    "if (not blnBatchMode):\n",
    "    utils.fnPlotTrainValLosses(lstTrainingStepLosses, lstValidationStepLosses, intValPerEpoch, argXLim = (), argYLim = ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data from files\n",
    "\n",
    "strCSVPath = argCSVPath\n",
    "\n",
    "fltResamplingFreq    = argResamplingFreq\n",
    "fltSubSeqDuration    = argSubSeqDuration\n",
    "intStepSizeTimePts   = argStepSizeTimePts\n",
    "dctStepSizeStates    = argStepSizeStates\n",
    "fltSubWindowFraction = argSubWindowFraction\n",
    "\n",
    "tupScalingParams     = argScalingParams\n",
    "\n",
    "blnShuffleData       = argShuffleData  # TODO: Should this follow the model or the argument?\n",
    "\n",
    "# Read CHB-MIT training data from files using sliding window\n",
    "lstLabeledTestFilenames, lstLabeledTestSegLabels, lstLabeledTestSegTypes, arrLabeledTestDataRaw, lstLabeledTestSegDurations, lstLabeledTestSamplingFreqs, lstLabeledTestChannels, lstLabeledTestSequences, lstLabeledTestSubSequences, lstLabeledTestSeizureDurations, arrLabeledTestStartEndTimesSec, _ = dio.fnReadCHBMITEDFFiles_SlidingWindow(\n",
    "    argCSVPath = strCSVPath, argResamplingFreq = fltResamplingFreq, argSubSeqDuration = fltSubSeqDuration, argScalingParams = tupScalingParams, argScalingInfo = tupScalingInfo, argStepSizeTimePts = intStepSizeTimePts, argStepSizeStates = dctStepSizeStates, argSubWindowFraction = fltSubWindowFraction, argAnnoSuffix = 'annotation.txt', argDebug = False, argTestMode = False)\n",
    "\n",
    "print()\n",
    "\n",
    "utils.fnShowMemUsage()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the channels used for training are the same as the one used for testing\n",
    "if (lstTrainingChannels):\n",
    "    if (lstTrainingChannels != lstLabeledTestChannels[0]):\n",
    "        print('Training channels:\\n  {}'.format(lstTrainingChannels))\n",
    "        print()\n",
    "        print('Test channels:\\n  {}'.format(lstLabeledTestChannels[0]))\n",
    "        \n",
    "        raise Exception('Training channels do not match test channels!')\n",
    "        \n",
    "# If tupScalingInfo exists (scaling across training and test sets, check whether the test\n",
    "# files used in scaling are the same as the ones specified in this test script\n",
    "if (tupScalingInfo):\n",
    "    lstScalingTestFilenames = sorted(tupScalingInfo[0])\n",
    "    lstScriptTestFilenames = sorted(set(lstLabeledTestFilenames))\n",
    "    \n",
    "    if (lstScalingTestFilenames != lstScriptTestFilenames):\n",
    "        print('Test files used for scaling:\\n {}'.format())\n",
    "        print()\n",
    "        print('Test files specified in test script:\\n {}'.format())\n",
    "        raise Exception('Test files used for scaling do not match test files specified in test script!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Read test data from files using the non-sliding window version of fnReadCHBMITEDFFiles()\n",
    "lstLabeledTestFilenames_Old, lstLabeledTestSegLabels_Old, lstLabeledTestSegTypes_Old, arrLabeledTestDataRaw_Old, lstLabeledTestSegDurations_Old, lstLabeledTestSamplingFreqs_Old, lstLabeledTestChannels_Old, lstLabeledTestSequences_Old, lstLabeledTestSubSequences_Old, lstLabeledTestSeizureDurations_Old, arrLabeledTestStartEndTimesSec_Old = dio.fnReadCHBMITEDFFiles(\n",
    "    argCSVPath = strCSVPath, argResamplingFreq = fltResamplingFreq, argSubSeqDuration = fltSubSeqDuration, argScalingParams = tupScalingParams, argAnnoSuffix = 'annotation.txt', argDebug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Compare results between fnReadCHBMITEDFFiles_SlidingWindow() and fnReadCHBMITEDFFiles().\n",
    "\n",
    "print('Same') if (lstLabeledTestFilenames        == lstLabeledTestFilenames_Old)                      else print('Different')\n",
    "print('Same') if (lstLabeledTestSegLabels        == lstLabeledTestSegLabels_Old)                      else print('Different')\n",
    "print('Same') if (lstLabeledTestSegTypes         == lstLabeledTestSegTypes_Old)                       else print('Different')\n",
    "\n",
    "#print('Same') if (np.array_equal(arrLabeledTestDataRaw, arrLabeledTestDataRaw_Old))                   else print('Different')\n",
    "if (np.array_equal(arrLabeledTestDataRaw, arrLabeledTestDataRaw_Old)):\n",
    "    print('Same')\n",
    "else:\n",
    "    print('Different (Difference = {})'.format(np.sum(arrLabeledTestDataRaw - arrLabeledTestDataRaw_Old)))\n",
    "\n",
    "print('Same') if (lstLabeledTestSegDurations     == lstLabeledTestSegDurations_Old)                   else print('Different')\n",
    "print('Same') if (lstLabeledTestSamplingFreqs    == lstLabeledTestSamplingFreqs_Old)                  else print('Different')\n",
    "print('Same') if (lstLabeledTestChannels         == lstLabeledTestChannels_Old)                       else print('Different')\n",
    "print('Same') if (lstLabeledTestSequences        == lstLabeledTestSequences_Old)                      else print('Different')\n",
    "print('Same') if (lstLabeledTestSubSequences     == lstLabeledTestSubSequences_Old)                   else print('Different')\n",
    "print('Same') if (lstLabeledTestSeizureDurations == lstLabeledTestSeizureDurations_Old)               else print('Different')\n",
    "print('Same') if (np.array_equal(arrLabeledTestStartEndTimesSec, arrLabeledTestStartEndTimesSec_Old)) else print('Different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Take a quick snapshot of the data set\n",
    "intStartIdx = 5000\n",
    "intEndIdx = 6010\n",
    "#intEndIdx = len(lstTrainingSegLabels)\n",
    "\n",
    "print('Displaying a snapshot of the data set (from subsequence {} to {}):\\n'.format(intStartIdx, intEndIdx))\n",
    "\n",
    "for tupZip in zip(list(range(len(lstLabeledTestSegLabels[intStartIdx:intEndIdx]))),\n",
    "                  lstLabeledTestFilenames[intStartIdx:intEndIdx],\n",
    "                  lstLabeledTestSegLabels[intStartIdx:intEndIdx],\n",
    "                  lstLabeledTestSegTypes[intStartIdx:intEndIdx],\n",
    "                  lstLabeledTestSegDurations[intStartIdx:intEndIdx],\n",
    "                  lstLabeledTestSamplingFreqs[intStartIdx:intEndIdx],\n",
    "                  lstLabeledTestSequences[intStartIdx:intEndIdx],\n",
    "                  lstLabeledTestSubSequences[intStartIdx:intEndIdx], \n",
    "                  arrLabeledTestStartEndTimesSec[intStartIdx:intEndIdx, :]\n",
    "                 ):\n",
    "    print(*tupZip, sep = '\\t')\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationships between UID, UUID, and EEG segments\n",
    "\n",
    "Unique IDs are assigned to each subsequence in the test set in order to trace prediction results back to the original EEG subsequences. UIDs are not unique among subsequences that are replicated/augmented (e.g. preictal subsequences), while UUIDs are unique for each subsequence in the test set.\n",
    "\n",
    "```\n",
    "Type Seq SubSeq SegIdx UID UUID\n",
    "---- --- ------ ------ --- ----\n",
    "Int   1   0      0      0   0\n",
    "Int   1   1      0      1   1\n",
    "Int   1   2      0      2   2\n",
    "\n",
    "Ict0  1   0      1      3   3\n",
    "Ict0  1   1      1      4   4\n",
    "Ict0  1   2      1      5   5\n",
    "\n",
    "Ict0  1   0      1      3   6\n",
    "Ict0  1   1      1      4   7\n",
    "Ict0  1   2      1      5   8\n",
    "\n",
    "Int   1   0      2      6   9\n",
    "Int   1   1      2      7   10\n",
    "Int   1   2      2      8   11\n",
    "\n",
    "Ict1  1   0      3      9   12\n",
    "Ict1  1   1      3      10  13\n",
    "Ict1  1   2      3      11  14\n",
    "\n",
    "Ict1  1   0      3      9   15\n",
    "Ict1  1   1      3      10  16\n",
    "Ict1  1   2      3      11  17\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign sequential unique ID (UID) to pre-augmented data\n",
    "print('Number of pre-augmentation subsequences = {}'.format(len(lstLabeledTestSegTypes)))\n",
    "print('arrLabeledTestDataRaw.shape = {}'.format(arrLabeledTestDataRaw.shape))\n",
    "\n",
    "lstLabeledTestUIDs = list(range(len(lstLabeledTestSegTypes)))\n",
    "print('lstLabeledTestUIDs = {} ... {}'.format(np.min(lstLabeledTestUIDs), np.max(lstLabeledTestUIDs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform non-random oversampling of the ictal data due to imbalanced\n",
    "# classification between the amount of interictal data versus ictal data\n",
    "\n",
    "blnDebug = False\n",
    "\n",
    "# Get the subsequences that are labeled as ictal state\n",
    "lstSeizureSeqIdx = [intSeqIdx for intSeqIdx, intSegType in enumerate(lstLabeledTestSegTypes) if intSegType == 2]\n",
    "\n",
    "# Collect all related data for these ictal subsequences\n",
    "lstSeizureFilenames     = list(itemgetter(*lstSeizureSeqIdx)(lstLabeledTestFilenames))\n",
    "lstSeizureSeqLabels     = list(itemgetter(*lstSeizureSeqIdx)(lstLabeledTestSegLabels))\n",
    "lstSeizureSegTypes      = list(itemgetter(*lstSeizureSeqIdx)(lstLabeledTestSegTypes))\n",
    "arrSeizureDataRaw       = arrLabeledTestDataRaw[:, :, lstSeizureSeqIdx]\n",
    "lstSeizureSegDurations  = list(itemgetter(*lstSeizureSeqIdx)(lstLabeledTestSegDurations))\n",
    "lstSeizureSamplingFreqs = list(itemgetter(*lstSeizureSeqIdx)(lstLabeledTestSamplingFreqs))\n",
    "lstSeizureChannels      = list(itemgetter(*lstSeizureSeqIdx)(lstLabeledTestChannels))\n",
    "lstSeizureSequences     = list(itemgetter(*lstSeizureSeqIdx)(lstLabeledTestSequences))\n",
    "lstSeizureSubSequences  = list(itemgetter(*lstSeizureSeqIdx)(lstLabeledTestSubSequences))\n",
    "\n",
    "arrSeizureStartEndTimesSec = arrLabeledTestStartEndTimesSec[lstSeizureSeqIdx, :]  # Start/end times (in seconds) for each subsequence\n",
    "\n",
    "lstSeizureUIDs          = list(itemgetter(*lstSeizureSeqIdx)(lstLabeledTestUIDs))  # Get UIDs of ictal subsequences\n",
    "\n",
    "print('arrSeizureDataRaw.shape = {}'.format(arrSeizureDataRaw.shape))\n",
    "print()\n",
    "\n",
    "if (blnDebug):\n",
    "    # Print info on the collected ictal subsequences\n",
    "    for tupSubSeq in zip(lstSeizureFilenames, lstSeizureSeqLabels, lstSeizureSegTypes, lstSeizureSegDurations, lstSeizureSamplingFreqs, lstSeizureSequences, lstSeizureSubSequences, lstSeizureUIDs):\n",
    "        print('{}, {}, {}, {}, {}, {}, {}, {}'.format(*tupSubSeq))\n",
    "\n",
    "    print()\n",
    "\n",
    "# Calculate the ratio between the number of ictal and non-ictal subsequences,\n",
    "# and multiply the number of ictal subsequences with this imbalance factor to\n",
    "# increase the presence of ictal subsequences in the data set\n",
    "intTotalSubSeqs = arrLabeledTestDataRaw.shape[2]\n",
    "intNumSeizureSubSeqs = arrSeizureDataRaw.shape[2]\n",
    "\n",
    "intImbalFactor = int(round((intTotalSubSeqs - intNumSeizureSubSeqs) / intNumSeizureSubSeqs))\n",
    "print('intTotalSubSeqs = {}, intNumSeizureSubSeqs = {}: intImbalFactor = {}'.format(intTotalSubSeqs, intNumSeizureSubSeqs, intImbalFactor))\n",
    "\n",
    "# Perform oversampling of ictal data only if intImbalFactor > 0\n",
    "if (intImbalFactor > 0):\n",
    "    print('Oversampling of ictal data performed using intImbalFactor = {}'.format(intImbalFactor))\n",
    "    print()\n",
    "    \n",
    "    # Multiply the number of ictal subsequences by intImbalFactor\n",
    "    lstSeizureFilenamesRep     = lstSeizureFilenames * intImbalFactor\n",
    "    lstSeizureSeqLabelsRep     = lstSeizureSeqLabels * intImbalFactor\n",
    "    lstSeizureSegTypesRep      = lstSeizureSegTypes * intImbalFactor\n",
    "    arrSeizureDataRawRep       = np.tile(arrSeizureDataRaw, (1, 1, intImbalFactor))\n",
    "    lstSeizureSegDurationsRep  = lstSeizureSegDurations * intImbalFactor\n",
    "    lstSeizureSamplingFreqsRep = lstSeizureSamplingFreqs * intImbalFactor\n",
    "    lstSeizureChannelsRep      = lstSeizureChannels * intImbalFactor\n",
    "    lstSeizureSequencesRep     = lstSeizureSequences * intImbalFactor\n",
    "    lstSeizureSubSequencesRep  = lstSeizureSubSequences * intImbalFactor\n",
    "\n",
    "    arrSeizureStartEndTimesSecRep = np.tile(arrSeizureStartEndTimesSec, (intImbalFactor, 1))  # Start/end times (in seconds) for each subsequence\n",
    "\n",
    "    lstSeizureUIDsRep  = lstSeizureUIDs * intImbalFactor\n",
    "\n",
    "    print('arrSeizureDataRawRep.shape = {}, arrSeizureStartEndTimesSec = {}'.format(arrSeizureDataRawRep.shape, arrSeizureStartEndTimesSec.shape))\n",
    "    print()\n",
    "\n",
    "    #for tupSubSeqRep in zip(lstSeizureFilenamesRep, lstSeizureSeqLabelsRep, lstSeizureSegTypesRep, lstSeizureSegDurationsRep, lstSeizureSamplingFreqsRep, lstSeizureSequencesRep, lstSeizureSubSequencesRep):\n",
    "    #    print('{}, {}, {}, {}, {}, {}, {}'.format(*tupSubSeqRep))\n",
    "\n",
    "    #print()\n",
    "\n",
    "    # Append the replicated ictal data to the end of the data set\n",
    "    lstLabeledTestFilenamesAug     = lstLabeledTestFilenames + lstSeizureFilenamesRep\n",
    "    lstLabeledTestSegLabelsAug     = lstLabeledTestSegLabels + lstSeizureSeqLabelsRep\n",
    "    lstLabeledTestSegTypesAug      = lstLabeledTestSegTypes + lstSeizureSegTypesRep\n",
    "    arrLabeledTestDataRawAug       = np.concatenate((arrLabeledTestDataRaw, arrSeizureDataRawRep), axis = 2)\n",
    "    lstLabeledTestSegDurationsAug  = lstLabeledTestSegDurations + lstSeizureSegDurationsRep\n",
    "    lstLabeledTestSamplingFreqsAug = lstLabeledTestSamplingFreqs + lstSeizureSamplingFreqsRep\n",
    "    lstLabeledTestChannelsAug      = lstLabeledTestChannels + lstSeizureChannelsRep\n",
    "    lstLabeledTestSequencesAug     = lstLabeledTestSequences + lstSeizureSequencesRep\n",
    "    lstLabeledTestSubSequencesAug  = lstLabeledTestSubSequences + lstSeizureSubSequencesRep\n",
    "\n",
    "    arrLabeledTestStartEndTimesSecAug  = np.concatenate((arrLabeledTestStartEndTimesSec, arrSeizureStartEndTimesSecRep), axis = 0)  # # Start/end times (in seconds) for each subsequence\n",
    "\n",
    "    lstLabeledTestUIDsAug          = lstLabeledTestUIDs + lstSeizureUIDsRep\n",
    "\n",
    "    print('arrLabeledTestDataRawAug.shape = {}'.format(arrLabeledTestDataRawAug.shape))\n",
    "\n",
    "    # Append the replicated ictal data to the end of the data set\n",
    "    lstLabeledTestFilenames     = lstLabeledTestFilenamesAug\n",
    "    lstLabeledTestSegLabels     = lstLabeledTestSegLabelsAug\n",
    "    lstLabeledTestSegTypes      = lstLabeledTestSegTypesAug\n",
    "    arrLabeledTestDataRaw       = arrLabeledTestDataRawAug\n",
    "    lstLabeledTestSegDurations  = lstLabeledTestSegDurationsAug\n",
    "    lstLabeledTestSamplingFreqs = lstLabeledTestSamplingFreqsAug\n",
    "    lstLabeledTestChannels      = lstLabeledTestChannelsAug\n",
    "    lstLabeledTestSequences     = lstLabeledTestSequencesAug\n",
    "    lstLabeledTestSubSequences  = lstLabeledTestSubSequencesAug\n",
    "\n",
    "    arrLabeledTestStartEndTimesSec = arrLabeledTestStartEndTimesSecAug  # Start/end times (in seconds) for each subsequence\n",
    "\n",
    "    lstLabeledTestUIDs          = lstLabeledTestUIDsAug\n",
    "\n",
    "    print()\n",
    "\n",
    "    print('Size of arrSeizureDataRawRep = {:.2f}Gb'.format(utils.fnByte2GB(arrSeizureDataRawRep.nbytes)))\n",
    "    print('Size of arrTrainingDataRaw   = {:.2f}Gb'.format(utils.fnByte2GB(arrLabeledTestDataRaw.nbytes)))\n",
    "    print()\n",
    "    \n",
    "else:\n",
    "    print('Oversampling of ictal data not performed since intImbalFactor = {}'.format(intImbalFactor))\n",
    "    print()\n",
    "    \n",
    "    print('Size of arrSeizureDataRaw = {:.2f}Gb'.format(utils.fnByte2GB(arrSeizureDataRaw.nbytes)))\n",
    "    print('Size of arrTrainingDataRaw   = {:.2f}Gb'.format(utils.fnByte2GB(arrTrainingDataRaw.nbytes)))\n",
    "    print()\n",
    "    \n",
    "utils.fnShowMemUsage()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign sequential unique unique ID (UUID) to post-augmented data\n",
    "lstLabeledTestUUIDs = list(range(len(lstLabeledTestUIDs)))\n",
    "print('lstLabeledTestUUIDs = {} ... {}'.format(np.min(lstLabeledTestUUIDs), np.max(lstLabeledTestUUIDs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input width = 15 (number of channels/features)\n",
    "# Sequence length = 600 * 5000 = 3000000 (number of time points)\n",
    "# Batch size = 1 (for each segment)\n",
    "\n",
    "# Reshape arrAllData[] from [feature/channel size x segment length x batch/segment size]\n",
    "# to batch_first [batch/segment size x segment length x feature/channel size]\n",
    "intLabeledTestNumChannels, intLabeledTestSeqLen, intLabeledTestNumSegments = arrLabeledTestDataRaw.shape\n",
    "print('intLabeledTestNumChannels, intLabeledTestSeqLen, intLabeledTestNumSegments = ({}, {}, {})'.format(intLabeledTestNumChannels, intLabeledTestSeqLen, intLabeledTestNumSegments))\n",
    "arrLabeledTestDataBatchFirst = arrLabeledTestDataRaw.T.reshape(intLabeledTestNumSegments, intLabeledTestSeqLen, intLabeledTestNumChannels)\n",
    "print('arrTestDataBatchFirst.shape = {}'.format(arrLabeledTestDataBatchFirst.shape))\n",
    "\n",
    "# Convert the segment types into an np.array\n",
    "arrLabeledTestSegTypes = np.array(lstLabeledTestSegTypes, dtype = int)\n",
    "print('arrLabeledTestSegTypes = {}'.format(arrLabeledTestSegTypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrLabeledTestData = arrLabeledTestDataBatchFirst\n",
    "arrLabeledTestLabels = arrLabeledTestSegTypes\n",
    "print('arrLabeledTestData.shape = {}, arrLabeledTestLabels.shape = {}'.format(arrLabeledTestData.shape, arrLabeledTestLabels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate arrLabeledTestUUIDs[] to arrLabeledTestLabels[] so their entries are\n",
    "# fed to the neural network in tandem during testing\n",
    "arrLabeledTestUUIDs = np.array(lstLabeledTestUUIDs, dtype = int)  # Convert from list to np.array\n",
    "print('arrLabeledTestLabels.shape = {}, arrLabeledTestUUIDs.shape = {}'.format(arrLabeledTestLabels.shape, arrLabeledTestUUIDs.shape))\n",
    "\n",
    "# Concatenate the two arrays vertically\n",
    "arrLabeledTestLabelsWithUUIDs = np.concatenate((arrLabeledTestLabels[:, np.newaxis], arrLabeledTestUUIDs[:, np.newaxis]), axis = 1)\n",
    "print('arrLabeledTestLabelsWithUUIDs.shape = {}'.format(arrLabeledTestLabelsWithUUIDs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data/labels into DataLoader objects so we can easily iterate through the\n",
    "# data sets during testing\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert test data and labels from np.arrays into data set wrapping tensors for DataLoader\n",
    "#objLabeledTestDataset = TensorDataset(torch.from_numpy(arrLabeledTestData), torch.from_numpy(arrLabeledTestLabels))\n",
    "objLabeledTestDataset = TensorDataset(torch.from_numpy(arrLabeledTestData), torch.from_numpy(arrLabeledTestLabelsWithUUIDs))  # Use labels with UUIDs attached\n",
    "objLabeledTestLoader  = DataLoader(objLabeledTestDataset, shuffle = blnShuffleData, batch_size = intBatchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available and if so, set a device to use\n",
    "\n",
    "intGPUDevice = argGPUDevice\n",
    "\n",
    "blnTrainOnGPU = torch.cuda.is_available()\n",
    "\n",
    "if (blnTrainOnGPU):\n",
    "    intNumGPUs = torch.cuda.device_count()\n",
    "    print('Training on GPU ({} available):'.format(intNumGPUs))\n",
    "    for intGPU in range(intNumGPUs):\n",
    "        print('  Device {}: {}'.format(intGPU, torch.cuda.get_device_name(intGPU)))\n",
    "    torch.cuda.set_device(intGPUDevice)\n",
    "    print('Using GPU #{}'.format(intGPUDevice))\n",
    "else:\n",
    "    print('No GPU available, training on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss criterion\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "objCriterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model with test data set and record test losses & prediction accuracy\n",
    "\n",
    "blnDebug = False\n",
    "lstTestLosses = []  # Record test losses per batch/step\n",
    "intNumCorrect = 0   # Number of correctly predicted sequences in a batch size of (intBatchSize)\n",
    "\n",
    "# Initialize arrTestResults[] to collect UUIDs, labels, and prediction results\n",
    "intBatchIdx = 0\n",
    "intNumBatches = arrLabeledTestLabelsWithUUIDs.shape[0] // intBatchSize  # Exclude the last orphan batch\n",
    "intTestSetSize = intNumBatches * intBatchSize\n",
    "arrTestResults = np.zeros((intTestSetSize, 3), dtype = int)\n",
    "if (blnDebug): print('arrTestResults.shape = {}'.format(arrTestResults.shape))\n",
    "\n",
    "# Initialize hidden and cell states\n",
    "arrHiddenState = objModelLSTM.initHidden(intBatchSize, blnTrainOnGPU, argDebug = False)  # Batch size defined above when creating DataLoader\n",
    "\n",
    "# Move the model to the GPU if one is available\n",
    "if (blnTrainOnGPU):\n",
    "    objModelLSTM.cuda()\n",
    "    \n",
    "objModelLSTM.eval()\n",
    "\n",
    "objTestLoader = objLabeledTestLoader\n",
    "\n",
    "# Batch loop (each loop trains one batch of input data)\n",
    "#for arrInputData, arrLabels in objTestLoader:\n",
    "for arrInputData, arrLabelsWithUUIDs in objTestLoader:  # Batches of labels are attached with their corresponding UUIDs\n",
    "    print('Feed forwarding new test batch (#{})...'.format(intBatchIdx + 1))\n",
    "    \n",
    "    # Separate labels from their UUIDs into different np.arrays\n",
    "    arrLabels = arrLabelsWithUUIDs[:, 0]\n",
    "    arrUUIDs   = arrLabelsWithUUIDs[:, 1]\n",
    "    \n",
    "    # If batch size allocated from DataLoader is smaller than intBatchSize\n",
    "    # (which happens on the last batch when the data set is not divisible\n",
    "    # by intBatchSize), break out of the loop\n",
    "    \n",
    "    # TODO: This is the strategy for now until we figure out what the best\n",
    "    #       strategy is on how/whether to initialize the hidden state with\n",
    "    #       a smaller batch size for the last orphan batch\n",
    "    if (intBatchSize != arrInputData.shape[0]):\n",
    "        arrUnusedUUIDs = utils.fnTensor2Array(arrUUIDs, blnTrainOnGPU)  # Convert tensor back to np.array\n",
    "        print('Exiting validation loop (intBatchSize = {}, arrInputData.shape[0] = {})'.format(intBatchSize, arrInputData.shape[0]))\n",
    "        break\n",
    "    else:\n",
    "        arrUnusedUUIDs = np.array([])\n",
    "        \n",
    "    if (blnTrainOnGPU):\n",
    "        arrInputData, arrLabels = arrInputData.cuda(), arrLabels.cuda()\n",
    "    \n",
    "    # Extract new variables for the hidden and cell states to decouple states\n",
    "    # from backprop history. Otherwise the gradient will be backpropagated\n",
    "    # through the entire training history\n",
    "    arrHiddenState = tuple([arrState.data for arrState in arrHiddenState])\n",
    "\n",
    "    # Forward pass through the model and get the next hidden state and output\n",
    "    # Output shape = (batch_size, 1), h shape = (n_layers, batch_size, hidden_dim)\n",
    "    arrOutput, arrHiddenState = objModelLSTM.forward(arrInputData, arrHiddenState, argDebug = False)\n",
    "    \n",
    "    if (blnDebug):\n",
    "        print('  arrLabels = {}'.format(arrLabels))\n",
    "        print('  arrOutput = \\n{}'.format(arrOutput))\n",
    "    \n",
    "    # Calculate test loss for this batch\n",
    "    fltTestLoss = objCriterion(arrOutput, arrLabels)\n",
    "    print('  fltTestLoss = {:.6f} ({})'.format(fltTestLoss, fltTestLoss.type()))\n",
    "    \n",
    "    # Record test loss for this batch\n",
    "    lstTestLosses.append(fltTestLoss.item())\n",
    "    \n",
    "    # Convert output scores between classes to one-hot encoding\n",
    "    # that indicate the predictions for each batch\n",
    "    _, arrPredictions = torch.max(arrOutput, 1)\n",
    "    \n",
    "    # Compare the class predictions to the test set labels\n",
    "    arrCorrect = arrPredictions.eq(arrLabels)\n",
    "    arrCorrect = utils.fnTensor2Array(arrCorrect, blnTrainOnGPU)  # Convert tensor back to np.array\n",
    "        \n",
    "    intNumCorrect += np.sum(arrCorrect)  # Count the number of correctly predicted sequences\n",
    "    \n",
    "    # Convert tensors back to np.arrays for UUIDs, labels, and prediction results\n",
    "    arrUUIDs = utils.fnTensor2Array(arrUUIDs, blnTrainOnGPU)\n",
    "    arrLabels = utils.fnTensor2Array(arrLabels, blnTrainOnGPU)\n",
    "    arrPredictions = utils.fnTensor2Array(arrPredictions, blnTrainOnGPU)\n",
    "        \n",
    "    arrBatchResults = np.concatenate((arrUUIDs[:, np.newaxis], arrLabels[:, np.newaxis], arrPredictions[:, np.newaxis]), axis = 1)\n",
    "    if (blnDebug): print('  arrBatchResults = \\n{}'.format(arrBatchResults))\n",
    "    \n",
    "    intStartIdx = intBatchIdx * intBatchSize\n",
    "    intEndIdx   = intStartIdx + intBatchSize\n",
    "    if (blnDebug): print('  intStartIdx = {}, intEndIdx = {}'.format(intStartIdx, intEndIdx))\n",
    "    \n",
    "    arrTestResults[intStartIdx:intEndIdx, :] = arrBatchResults  # Place batch results into arrTestResults[]\n",
    "    \n",
    "    intBatchIdx = intBatchIdx + 1\n",
    "    \n",
    "    # Clear the GPU cache regularly to avoid the following CUDA error:\n",
    "    #\n",
    "    #   RuntimeError: CUDA out of memory. Tried to allocate 6.61 GiB \n",
    "    #   (GPU 1; 10.76 GiB total capacity; 1.25 GiB already allocated; \n",
    "    #   2.39 GiB free; 6.38 GiB cached)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Print the mean test loss for the entire test set\n",
    "print(\"Test Loss = {:.4f}\".format(np.mean(lstTestLosses)))\n",
    "\n",
    "# Print the test accuracy over all test data\n",
    "fltTestAccuracy = intNumCorrect / intTestSetSize\n",
    "print(\"Test Accuracy = {}/{} = {:.4f}\".format(int(round(intNumCorrect)), intTestSetSize, fltTestAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform post-testing errror analyses (for augmented data set)\n",
    "\n",
    "# Lists and arrays that may be helpful with the error analysis:\n",
    "#\n",
    "#  lstLabeledTestFilenames\n",
    "#  lstLabeledTestSegLabels\n",
    "#  lstLabeledTestSegTypes\n",
    "#  lstLabeledTestSequences\n",
    "#  lstLabeledTestSubSequences\n",
    "\n",
    "#  lstLabeledTestUIDs\n",
    "#  lstLabeledTestUUIDs\n",
    "\n",
    "# Lists that are already converted to np.arrays:\n",
    "#\n",
    "#  arrLabeledTestSegTypes\n",
    "#  arrLabeledTestUUIDs\n",
    "#\n",
    "#  arrLabeledTestStartEndTimesSec\n",
    "\n",
    "intNumUnusedUUIDs = arrUnusedUUIDs.shape[0]\n",
    "\n",
    "print('Number of unused UUIDs in orphan batch = {}'.format(intNumUnusedUUIDs))\n",
    "if (blnDebug): print('Unused UUIDs in orphan batch (sorted):\\n{}'.format(arrUnusedUUIDs[arrUnusedUUIDs.argsort()]))\n",
    "\n",
    "# Convert lists into np.arrays\n",
    "arrLabeledTestFilnames     = np.array(lstLabeledTestFilenames)\n",
    "arrLabeledTestSegLabels    = np.array(lstLabeledTestSegLabels)\n",
    "arrLabeledTestSequences    = np.array(lstLabeledTestSequences)\n",
    "arrLabeledTestSubSequences = np.array(lstLabeledTestSubSequences)\n",
    "arrLabeledTestUIDs         = np.array(lstLabeledTestUIDs)\n",
    "\n",
    "if (intNumUnusedUUIDs):\n",
    "    # Remove unused entries from orphan batch based on their UUIDs\n",
    "    arrLabeledTestFilnames_Tested     = np.delete(arrLabeledTestFilnames, arrUnusedUUIDs)\n",
    "    arrLabeledTestSegLabels_Tested    = np.delete(arrLabeledTestSegLabels, arrUnusedUUIDs)\n",
    "    arrLabeledTestSegTypes_Tested     = np.delete(arrLabeledTestSegTypes, arrUnusedUUIDs)\n",
    "    arrLabeledTestSequences_Tested    = np.delete(arrLabeledTestSequences, arrUnusedUUIDs)\n",
    "    arrLabeledTestSubSequences_Tested = np.delete(arrLabeledTestSubSequences, arrUnusedUUIDs)\n",
    "    arrLabeledTestUIDs_Tested         = np.delete(arrLabeledTestUIDs, arrUnusedUUIDs)\n",
    "    arrLabeledTestUUIDs_Tested        = np.delete(arrLabeledTestUUIDs, arrUnusedUUIDs)\n",
    "\n",
    "    arrLabeledTestStartEndTimesSec_Tested = np.delete(arrLabeledTestStartEndTimesSec, arrUnusedUUIDs, axis = 0)  # Start/end times (in seconds) for each subsequence\n",
    "\n",
    "else:\n",
    "    arrLabeledTestFilnames_Tested     = arrLabeledTestFilnames\n",
    "    arrLabeledTestSegLabels_Tested    = arrLabeledTestSegLabels\n",
    "    arrLabeledTestSegTypes_Tested     = arrLabeledTestSegTypes\n",
    "    arrLabeledTestSequences_Tested    = arrLabeledTestSequences\n",
    "    arrLabeledTestSubSequences_Tested = arrLabeledTestSubSequences\n",
    "    arrLabeledTestUIDs_Tested         = arrLabeledTestUIDs\n",
    "    arrLabeledTestUUIDs_Tested        = arrLabeledTestUUIDs\n",
    "\n",
    "    arrLabeledTestStartEndTimesSec_Tested = arrLabeledTestStartEndTimesSec  # Start/end times (in seconds) for each subsequence\n",
    "    \n",
    "# Sort arrTestResults[] based on their UUIDs since the test entries are shuffled\n",
    "# before being fed into the neural network\n",
    "arrTestResults_Sorted = arrTestResults[arrTestResults[:, 0].argsort()]  # Columns = [UUID, label, predicition]\n",
    "print('arrTestResults_Sorted.shape = {}'.format(arrTestResults_Sorted.shape))\n",
    "print()\n",
    "\n",
    "# Generate masks for various metrics (for augmented data set)\n",
    "arrFalsePositivesMask = np.logical_and(\n",
    "    arrTestResults_Sorted[:, 1] == dio.dctSegStates['interictal'][1], arrTestResults_Sorted[:, 2] == dio.dctSegStates['ictal'][1])       # False positive (interictal (0) -> ictal (2))\n",
    "arrFalseNegativesMask = np.logical_and(\n",
    "    arrTestResults_Sorted[:, 1] == dio.dctSegStates['ictal'][1], arrTestResults_Sorted[:, 2] == dio.dctSegStates['interictal'][1])       # False negative (ictal (2) -> interictal (0))\n",
    "arrTruePositivesMask  = np.logical_and(\n",
    "    arrTestResults_Sorted[:, 1] == dio.dctSegStates['ictal'][1], arrTestResults_Sorted[:, 2] == dio.dctSegStates['ictal'][1])            # True positive (ictal (2) -> ictal (2))\n",
    "arrTrueNegativesMask  = np.logical_and(\n",
    "    arrTestResults_Sorted[:, 1] == dio.dctSegStates['interictal'][1], arrTestResults_Sorted[:, 2] == dio.dctSegStates['interictal'][1])  # True negative (interictal (0) -> interictal (0))\n",
    "\n",
    "intTestSetSize = arrTestResults_Sorted.shape[0]\n",
    "\n",
    "# Calculate various metrics (for augmented data set)\n",
    "intNumFalsePositives = arrTestResults_Sorted[arrFalsePositivesMask].shape[0]\n",
    "intNumFalseNegatives = arrTestResults_Sorted[arrFalseNegativesMask].shape[0]\n",
    "intNumTruePositives  = arrTestResults_Sorted[arrTruePositivesMask].shape[0]\n",
    "intNumTrueNegatives  = arrTestResults_Sorted[arrTrueNegativesMask].shape[0]\n",
    "\n",
    "intNumCorrect = intTestSetSize - intNumFalsePositives - intNumFalseNegatives\n",
    "\n",
    "print('intTestSetSize = {}, intNumFalsePositives = {}, intNumFalseNegatives = {}, intNumTruePositives = {}, intNumTrueNegatives = {}'\n",
    "      .format(intTestSetSize, intNumFalsePositives, intNumFalseNegatives, intNumTruePositives, intNumTrueNegatives))\n",
    "\n",
    "fltTruePositiveRate, fltTrueNegativeRate = utils.fnCalcPerfMetrics(\n",
    "    intNumFalsePositives, intNumFalseNegatives, intNumTruePositives, intNumTrueNegatives)\n",
    "\n",
    "# Print the test accuracy over all test data (sanity check) and other performance metrics\n",
    "fltTestAccuracy = intNumCorrect / intTestSetSize\n",
    "print('Test Accuracy = {}/{} = {:.4f}'.format(int(round(intNumCorrect)), intTestSetSize, fltTestAccuracy))\n",
    "print('fltTruePositiveRate = {:.4f}, fltTrueNegativeRate = {:.4f}'.format(fltTruePositiveRate, fltTrueNegativeRate))\n",
    "print()\n",
    "\n",
    "# Visualize test results in a tabular format, including the other related lists/arrays (for augmented data set)\n",
    "\n",
    "# NOTE: Since some of the fields (e.g. filenames and labels) are in string format, the\n",
    "#       entire table is converted to string, including the sequence numbers and unique\n",
    "#       ID. Therefore, arrCompleteTestResults[] is for visualization only. All other\n",
    "#       types of operations should be performed on the uncombined lists/arrays\n",
    "arrCompleteTestResults = np.concatenate((arrLabeledTestFilnames_Tested[:, np.newaxis], \n",
    "                                         arrLabeledTestSegLabels_Tested[:, np.newaxis], \n",
    "                                         arrLabeledTestSegTypes_Tested[:, np.newaxis], \n",
    "                                         arrLabeledTestSequences_Tested[:, np.newaxis], \n",
    "                                         arrLabeledTestSubSequences_Tested[:, np.newaxis], \n",
    "                                         arrLabeledTestUIDs_Tested[:, np.newaxis], \n",
    "                                         arrLabeledTestUUIDs_Tested[:, np.newaxis], \n",
    "                                         arrLabeledTestStartEndTimesSec_Tested, \n",
    "                                         arrTestResults_Sorted\n",
    "                                        ), axis = 1)\n",
    "print('arrCompleteTestResults.shape = {}'.format(arrCompleteTestResults.shape))\n",
    "print()\n",
    "\n",
    "print('Entries with false positives (interictal predicted as ictal):\\n{}'.format(arrCompleteTestResults[arrFalsePositivesMask][0:10, :]))\n",
    "print()\n",
    "\n",
    "print('Entries with false negatives (ictal predicted as interictal):\\n{}'.format(arrCompleteTestResults[arrFalseNegativesMask][0:10, :]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform post-testing errror analyses (for non-augmented data set)\n",
    "\n",
    "# Find the list of unique UID entries, which will exclude any augmented entries\n",
    "arrLabeledTestUIDsOut_TestedNoAug, arrLabeledTestUIDsIdx_TestedNoAug, arrLabeledTestUIDsCount_TestedNoAug = np.unique(\n",
    "    arrLabeledTestUIDs_Tested, return_index = True, return_inverse = False, return_counts = True, axis = 0)\n",
    "print('np.unique() results = {}, {}, {}'.format(arrLabeledTestUIDsOut_TestedNoAug.shape, arrLabeledTestUIDsIdx_TestedNoAug.shape, arrLabeledTestUIDsCount_TestedNoAug.shape))\n",
    "print()\n",
    "\n",
    "# Remove replicated augmented entries based on their UUIDs by keeping only the unique entries\n",
    "arrLabeledTestFilnames_TestedNoAug     = arrLabeledTestFilnames_Tested[arrLabeledTestUIDsIdx_TestedNoAug]\n",
    "arrLabeledTestSegLabels_TestedNoAug    = arrLabeledTestSegLabels_Tested[arrLabeledTestUIDsIdx_TestedNoAug]\n",
    "arrLabeledTestSegTypes_TestedNoAug     = arrLabeledTestSegTypes_Tested[arrLabeledTestUIDsIdx_TestedNoAug]\n",
    "arrLabeledTestSequences_TestedNoAug    = arrLabeledTestSequences_Tested[arrLabeledTestUIDsIdx_TestedNoAug]\n",
    "arrLabeledTestSubSequences_TestedNoAug = arrLabeledTestSubSequences_Tested[arrLabeledTestUIDsIdx_TestedNoAug]\n",
    "arrLabeledTestUIDs_TestedNoAug         = arrLabeledTestUIDs_Tested[arrLabeledTestUIDsIdx_TestedNoAug]\n",
    "arrLabeledTestUUIDs_TestedNoAug        = arrLabeledTestUUIDs_Tested[arrLabeledTestUIDsIdx_TestedNoAug]\n",
    "\n",
    "arrLabeledTestStartEndTimesSec_TestedNoAug = arrLabeledTestStartEndTimesSec_Tested[arrLabeledTestUIDsIdx_TestedNoAug, :]\n",
    "\n",
    "arrTestResults_SortedNoAug = arrTestResults_Sorted[arrLabeledTestUIDsIdx_TestedNoAug, :]\n",
    "\n",
    "print('Are arrLabeledTestUIDsOut_TestedNoAug[] and arrLabeledTestUIDs_TestedNoAug[] identical? -> {}'.format(\n",
    "    np.array_equal(arrLabeledTestUIDsOut_TestedNoAug, arrLabeledTestUIDs_TestedNoAug)))\n",
    "print()\n",
    "\n",
    "# Generate masks for various metrics (for non-augmented data set)\n",
    "arrFalsePositivesMask_NoAug = np.logical_and(\n",
    "    arrTestResults_SortedNoAug[:, 1] == dio.dctSegStates['interictal'][1], arrTestResults_SortedNoAug[:, 2] == dio.dctSegStates['ictal'][1])       # False positive (interictal (0) -> ictal (2))\n",
    "arrFalseNegativesMask_NoAug = np.logical_and(\n",
    "    arrTestResults_SortedNoAug[:, 1] == dio.dctSegStates['ictal'][1], arrTestResults_SortedNoAug[:, 2] == dio.dctSegStates['interictal'][1])       # False negative (ictal (2) -> interictal (0))\n",
    "arrTruePositivesMask_NoAug  = np.logical_and(\n",
    "    arrTestResults_SortedNoAug[:, 1] == dio.dctSegStates['ictal'][1], arrTestResults_SortedNoAug[:, 2] == dio.dctSegStates['ictal'][1])            # True positive (ictal (2) -> ictal (2))\n",
    "arrTrueNegativesMask_NoAug  = np.logical_and(\n",
    "    arrTestResults_SortedNoAug[:, 1] == dio.dctSegStates['interictal'][1], arrTestResults_SortedNoAug[:, 2] == dio.dctSegStates['interictal'][1])  # True negative (interictal (0) -> interictal (0))\n",
    "\n",
    "intTestSetSize_NoAug = arrTestResults_SortedNoAug.shape[0]\n",
    "\n",
    "# Calculate various metrics (for non-augmented data set)\n",
    "intNumFalsePositives_NoAug = arrTestResults_SortedNoAug[arrFalsePositivesMask_NoAug].shape[0]\n",
    "intNumFalseNegatives_NoAug = arrTestResults_SortedNoAug[arrFalseNegativesMask_NoAug].shape[0]\n",
    "intNumTruePositives_NoAug  = arrTestResults_SortedNoAug[arrTruePositivesMask_NoAug].shape[0]\n",
    "intNumTrueNegatives_NoAug  = arrTestResults_SortedNoAug[arrTrueNegativesMask_NoAug].shape[0]\n",
    "\n",
    "intNumCorrect_NoAug = intTestSetSize_NoAug - intNumFalsePositives_NoAug - intNumFalseNegatives_NoAug\n",
    "\n",
    "print('intTestSetSize_NoAug = {}, intNumFalsePositives_NoAug = {}, intNumFalseNegatives_NoAug = {}, intNumTruePositives_NoAug = {}, intNumTrueNegatives_NoAug_NoAug = {}'\n",
    "      .format(intTestSetSize_NoAug, intNumFalsePositives_NoAug, intNumFalseNegatives_NoAug, intNumTruePositives_NoAug, intNumTrueNegatives_NoAug))\n",
    "\n",
    "fltTruePositiveRate_NoAug, fltTrueNegativeRate_NoAug = utils.fnCalcPerfMetrics(\n",
    "    intNumFalsePositives_NoAug, intNumFalseNegatives_NoAug, intNumTruePositives_NoAug, intNumTrueNegatives_NoAug)\n",
    "\n",
    "# Print the test accuracy over all test data (sanity check) and other performance metrics\n",
    "fltTestAccuracy_NoAug = intNumCorrect_NoAug / intTestSetSize_NoAug\n",
    "print('Test Accuracy (NoAug) = {}/{} = {:.4f}'.format(int(round(intNumCorrect_NoAug)), intTestSetSize_NoAug, fltTestAccuracy_NoAug))\n",
    "print('fltTruePositiveRate_NoAug = {:.4f}, fltTrueNegativeRate_NoAug = {:.4f}'.format(fltTruePositiveRate_NoAug, fltTrueNegativeRate_NoAug))\n",
    "print()\n",
    "\n",
    "# Visualize test results in a tabular format, including the other related lists/arrays (for non-augmented data set)\n",
    "arrCompleteTestResults_NoAug = np.concatenate((arrLabeledTestFilnames_TestedNoAug[:, np.newaxis], \n",
    "                                               arrLabeledTestSegLabels_TestedNoAug[:, np.newaxis], \n",
    "                                               arrLabeledTestSegTypes_TestedNoAug[:, np.newaxis], \n",
    "                                               arrLabeledTestSequences_TestedNoAug[:, np.newaxis], \n",
    "                                               arrLabeledTestSubSequences_TestedNoAug[:, np.newaxis], \n",
    "                                               arrLabeledTestUIDs_TestedNoAug[:, np.newaxis], \n",
    "                                               arrLabeledTestUUIDs_TestedNoAug[:, np.newaxis], \n",
    "                                               arrLabeledTestStartEndTimesSec_TestedNoAug, \n",
    "                                               arrTestResults_SortedNoAug\n",
    "                                              ), axis = 1)\n",
    "print('arrCompleteTestResults_NoAug.shape = {}'.format(arrCompleteTestResults_NoAug.shape))\n",
    "print()\n",
    "\n",
    "print('Entries with false positives (interictal predicted as ictal):\\n{}'.format(arrCompleteTestResults_NoAug[arrFalsePositivesMask_NoAug, :][0:100, :]))\n",
    "print()\n",
    "\n",
    "print('Entries with false negatives (ictal predicted as interictal):\\n{}'.format(arrCompleteTestResults_NoAug[arrFalseNegativesMask_NoAug, :][0:100, :]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (argSaveAnno):\n",
    "    # Save test results to annotation files for visualization in EDFbrowser\n",
    "    strTestAnnoPath = './SavedAnno/' + strTimestamp + '/'\n",
    "    print('Saving test result annotation files to: {}'.format(strTestAnnoPath))\n",
    "    print()\n",
    "    \n",
    "    dio.fnWriteTestAnnoFiles(\n",
    "        arrLabeledTestFilnames_TestedNoAug, arrTestResults_SortedNoAug, arrLabeledTestStartEndTimesSec_TestedNoAug, arrFalsePositivesMask_NoAug, arrFalseNegativesMask_NoAug, strTestAnnoPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (blnBatchMode):\n",
    "    # Close the log file and redirect output back to stdout and stderr\n",
    "    datScriptEnd = utils.fnNow()\n",
    "    print('Script ended on {}'.format(utils.fnGetDatetime(datScriptEnd)))\n",
    "\n",
    "    datScriptDuration = datScriptEnd - datScriptStart\n",
    "    print('datScriptDuration = {}'.format(datScriptDuration))\n",
    "\n",
    "    objLogFile.close()\n",
    "    sys.stdout = objStdout\n",
    "    sys.stderr = objStderr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
