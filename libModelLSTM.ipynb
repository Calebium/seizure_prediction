{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create an LSTM model that will be used to analyze multichannel EEG signal\n",
    "'''\n",
    "\n",
    "class clsLSTM(nn.Module):\n",
    "    # Initialize the model by setting up the layers\n",
    "    def __init__(self, argFeaturesDim, argHiddenDim, argNumLayers, argOutputSize, argDropProb = 0.5, argDebug = False):\n",
    "        super(clsLSTM, self).__init__()\n",
    "        \n",
    "        # Make these parameters accessible in the other methods\n",
    "        self.intFeaturesDim = argFeaturesDim\n",
    "        self.intHiddenDim   = argHiddenDim\n",
    "        self.intNumLayers   = argNumLayers\n",
    "        self.intOutputSize  = argOutputSize\n",
    "        self.fltDropProb    = argDropProb\n",
    "        \n",
    "        # Define the structure of each layer in the model\n",
    "        self.LSTMLayer = nn.LSTM(argFeaturesDim, argHiddenDim, argNumLayers,  # LSTM layer\n",
    "                                 dropout = argDropProb, batch_first = True)\n",
    "        \n",
    "        self.DropoutLayer = nn.Dropout(p = argDropProb)                       # Dropout layer\n",
    "        \n",
    "        self.FCLayer = nn.Linear(argHiddenDim, argOutputSize)                 # Fully-connected layer\n",
    "        \n",
    "        \n",
    "    # Display all the named parameters and their shapes in the model\n",
    "    def showParams(self):\n",
    "        intParamIdx = 0\n",
    "        \n",
    "        for tupParam in self.named_parameters():\n",
    "            print('{}: {} -> {}'.format(intParamIdx, tupParam[0], tupParam[1].data.shape))\n",
    "            intParamIdx = intParamIdx + 1            \n",
    "            \n",
    "            \n",
    "    # Perform a forward pass on the model provided with input data and a previous\n",
    "    # hidden state\n",
    "    def forward(self, argDataIn, argHiddenIn, argDebug = False):        \n",
    "        # NOTE: intBatchSize should be based on the first dimension of the input\n",
    "        #       data, not by the intBatchSize defined when formatting data (this\n",
    "        #       way we can feed in data with any batch size for testing). Otherwise,\n",
    "        #       things may work during training, but will break during testing,\n",
    "        #       when we feed in test data with different batch sizes (which is\n",
    "        #       perfectly legal)\n",
    "        intBatchSize = argDataIn.shape[0]\n",
    "        \n",
    "        # arrDataIn (batch size x sequence length, features dim) -> input data\n",
    "        # arrHiddenIn/Out (num layers, batch size, hidden dim) -> hidden state\n",
    "        # arrLSTMOut (batch size, sequence length, hidden dim) -> LSTM output\n",
    "        # arrFCOut (batch size * sequence length, output size) -> FC layer output\n",
    "        # arrOutput (batch_size, output_size) -> final output\n",
    "        \n",
    "        # Feed input data and the previous hidden state through the LSTM\n",
    "        # (just like passing input date through a CNN)\n",
    "        #arrLSTMOut, arrHiddenOut = self.LSTMLayer(argDataIn, argHiddenIn)\n",
    "        arrLSTMOut, arrHiddenOut = self.LSTMLayer(argDataIn.float(), argHiddenIn)\n",
    "        if (argDebug): print('arrLSTMOut.shape = {}'.format(arrLSTMOut.shape))\n",
    "        \n",
    "        # Pass through a dropout layer\n",
    "        arrDropoutOut = self.DropoutLayer(arrLSTMOut)\n",
    "        if (argDebug): print('arrDropoutOut.shape = {}'.format(arrDropoutOut.shape))\n",
    "        \n",
    "        # Reshape output to be [batch size * sequence length, hidden dim]\n",
    "        #arrDropoutFlat = arrDropoutOut.view(-1, self.intHiddenDim)\n",
    "        arrDropoutFlat = arrDropoutOut.contiguous().view(-1, self.intHiddenDim)\n",
    "        if (argDebug): print('arrDropoutFlat.shape = {}'.format(arrDropoutFlat.shape))\n",
    "        \n",
    "        # Put the flattened LSTM output through a fully-connected layer to get final\n",
    "        # output so arrFCOut[] will be [batch size * sequence length, output size]\n",
    "        arrFCOut = self.FCLayer(arrDropoutFlat)\n",
    "        if (argDebug): print('arrFCOut.shape = {}'.format(arrFCOut.shape))\n",
    "        \n",
    "        # Reshape the FC layer output to be [batch size, sequence length, output size]\n",
    "        arrFCOutReshaped = arrFCOut.view(intBatchSize, -1, self.intOutputSize)\n",
    "        if (argDebug): print('arrFCOutReshaped.shape = {}'.format(arrFCOutReshaped.shape))\n",
    "                \n",
    "        # Get the last output for each sequence, with shape becoming [batch size, output size]\n",
    "        arrOutput = arrFCOutReshaped[:, -1, :]\n",
    "        if (argDebug): print('arrOutput.shape = {}'.format(arrOutput.shape))\n",
    "                \n",
    "        # Return the last output of each sequence and a hidden state\n",
    "        return arrOutput, arrHiddenOut\n",
    "    \n",
    "    \n",
    "    # Initialize the hidden and cell states with zeros\n",
    "    def initHidden(self, argBatchSize, argTrainOnGPU = False, argDebug = False):\n",
    "        # Create two new tensors with sizes num layers x batch size x hidden dim,\n",
    "        # initialized to zero, for both hidden state and cell state of the LSTM\n",
    "        \n",
    "        # Hidden state carries the short-term memory and cell state carries the\n",
    "        # long-term memory:\n",
    "        #   https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "        arrWeight = next(self.parameters()).data  # NOTE: Apparently self.parameters() is an iterator. But why\n",
    "                                                  #       only get the next item? This is just so that we can\n",
    "                                                  #       use new() to create the hidden & cell states with the\n",
    "                                                  #       same dtype (but different shape)\n",
    "        if (argDebug):\n",
    "            print('arrWeight.shape = {} ({})'.format(arrWeight.shape, arrWeight.type()))\n",
    "            \n",
    "        # Hidden is a tuple of two tensors (hidden state, cell state) of the same shape (n_layers x\n",
    "        # batch_size x hidden_dim) create using new() with the same dtype as weight, initialized to\n",
    "        # zero_() in-place. The tuple requirement is documented in the PyTorch LSTM documentation\n",
    "        # as (h_0, c_0)\n",
    "        #   - https://pytorch.org/docs/stable/nn.html#lstm\n",
    "        #   - https://pytorch.org/docs/0.3.1/tensors.html?highlight=new#torch.Tensor.new\n",
    "        if (argTrainOnGPU):\n",
    "            arrHiddenState = (arrWeight.new(self.intNumLayers, argBatchSize, self.intHiddenDim).zero_().cuda(),\n",
    "                              arrWeight.new(self.intNumLayers, argBatchSize, self.intHiddenDim).zero_().cuda())\n",
    "        else:\n",
    "            arrHiddenState = (arrWeight.new(self.intNumLayers, argBatchSize, self.intHiddenDim).zero_(),\n",
    "                              arrWeight.new(self.intNumLayers, argBatchSize, self.intHiddenDim).zero_())\n",
    "        \n",
    "        if (argDebug):\n",
    "            print('intNumLayers = {}, argBatchSize = {}, intHiddenDim = {}'.format(self.intNumLayers, argBatchSize, self.intHiddenDim))\n",
    "            print('arrHiddenState.shape = ({}, {})'.format(arrHiddenState[0].shape, arrHiddenState[1].shape))\n",
    "            \n",
    "        return arrHiddenState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnSaveLSTMModel(argModelDir, argModelName, argNumChannels, argSeqLen, argNumSegments, argModel, \n",
    "                    argNumEpochs, argBatchSize, argShuffleIndices, argShuffleData, argLearningRate, argPrintEvery, argGradClip, \n",
    "                    argTrainingStepLosses, argValidationStepLosses, argDebug = False, **argModelProperties):\n",
    "    print('Saving model: argModelName = {}'.format(argModelName))\n",
    "        \n",
    "    dctModelCheckPt = {\n",
    "        'intNumChannels':          argNumChannels,\n",
    "        'intSeqLen':               argSeqLen,\n",
    "        'intNumSegments':          argNumSegments,\n",
    "\n",
    "        'intFeaturesDim':          argModel.intFeaturesDim,\n",
    "        'intHiddenDim':            argModel.intHiddenDim,\n",
    "        'intNumLayers':            argModel.intNumLayers,\n",
    "        'intOutputSize':           argModel.intOutputSize,\n",
    "        'fltDropProb':             argModel.fltDropProb,\n",
    "        'dctStateDict':            argModel.state_dict(),\n",
    "\n",
    "        'intNumEpochs':            argNumEpochs,        # TODO: Replicated in dctModelProperties (not removed for backwards-compatibility)\n",
    "        'intBatchSize':            argBatchSize,        # TODO: Replicated in dctModelProperties (not removed for backwards-compatibility)\n",
    "        'blnShuffleIndices':       argShuffleIndices,   # TODO: Replicated in dctModelProperties (not removed for backwards-compatibility)\n",
    "        'blnShuffleData':          argShuffleData,      # TODO: Replicated in dctModelProperties (not removed for backwards-compatibility)\n",
    "        'fltLearningRate':         argLearningRate,     # TODO: Replicated in dctModelProperties (not removed for backwards-compatibility)\n",
    "        'intPrintEvery':           argPrintEvery,       # TODO: Replicated in dctModelProperties (not removed for backwards-compatibility)\n",
    "        'fltGradClip':             argGradClip,         # TODO: Replicated in dctModelProperties (not removed for backwards-compatibility)\n",
    "\n",
    "        'lstTrainingStepLosses':   argTrainingStepLosses,\n",
    "        'lstValidationStepLosses': argValidationStepLosses,\n",
    "        \n",
    "        'dctModelProperties':      argModelProperties   # Important training and model-specific parameters        \n",
    "    }\n",
    "    \n",
    "    if (argDebug): print(dctModelCheckPt)\n",
    "    \n",
    "    # Save model to file system with 'write' and 'binary' options\n",
    "    with open(argModelDir + argModelName, 'wb') as objModelFile:\n",
    "        torch.save(dctModelCheckPt, objModelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnLoadLSTMModel(argModelDir, argModelName, argDebug = False):\n",
    "    print('Loading model: argModelName = {}'.format(argModelName))\n",
    "    \n",
    "    with open(argModelDir + argModelName, 'rb') as objModelFile:\n",
    "        dctModelCheckPt = torch.load(objModelFile)\n",
    "\n",
    "    # Extract saved parameters from the model file\n",
    "    intNumChannels          = dctModelCheckPt['intNumChannels']\n",
    "    intSeqLen               = dctModelCheckPt['intSeqLen']\n",
    "    intNumSegments          = dctModelCheckPt['intNumSegments']\n",
    "\n",
    "    intFeaturesDim          = dctModelCheckPt['intFeaturesDim']\n",
    "    intHiddenDim            = dctModelCheckPt['intHiddenDim']\n",
    "    intNumLayers            = dctModelCheckPt['intNumLayers']\n",
    "    intOutputSize           = dctModelCheckPt['intOutputSize']\n",
    "    fltDropProb             = dctModelCheckPt['fltDropProb']\n",
    "    dctStateDict            = dctModelCheckPt['dctStateDict']\n",
    "\n",
    "    intNumEpochs            = dctModelCheckPt['intNumEpochs']\n",
    "    intBatchSize            = dctModelCheckPt['intBatchSize']\n",
    "    blnShuffleIndices       = dctModelCheckPt['blnShuffleIndices']\n",
    "    blnShuffleData          = dctModelCheckPt['blnShuffleData']\n",
    "    fltLearningRate         = dctModelCheckPt['fltLearningRate']\n",
    "    intPrintEvery           = dctModelCheckPt['intPrintEvery']\n",
    "    fltGradClip             = dctModelCheckPt['fltGradClip']\n",
    "\n",
    "    lstTrainingStepLosses   = dctModelCheckPt['lstTrainingStepLosses']\n",
    "    lstValidationStepLosses = dctModelCheckPt['lstValidationStepLosses']\n",
    "    \n",
    "    # Return dctModelProperties{} if it exists. Otherwise, return an empty dictionary\n",
    "    if ('dctModelProperties' in dctModelCheckPt.keys()):\n",
    "        dctModelProperties  = dctModelCheckPt['dctModelProperties']\n",
    "        blnHasModelProperties = True\n",
    "    else:\n",
    "        dctModelProperties  = {}\n",
    "        blnHasModelProperties = False\n",
    "    \n",
    "    # Reconstruct the LSTM model with the saved parameters\n",
    "    objModelLSTM = clsLSTM(intFeaturesDim, intHiddenDim, intNumLayers, intOutputSize, argDropProb = fltDropProb)\n",
    "    objModelLSTM.load_state_dict(dctStateDict)\n",
    "    \n",
    "    return (intNumChannels, intSeqLen, intNumSegments, objModelLSTM,\n",
    "            intNumEpochs, intBatchSize, blnShuffleIndices, blnShuffleData, fltLearningRate, intPrintEvery, fltGradClip,\n",
    "            lstTrainingStepLosses, lstValidationStepLosses, dctModelProperties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
